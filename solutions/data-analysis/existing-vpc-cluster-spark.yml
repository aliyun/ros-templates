ROSTemplateFormatVersion: '2015-09-01'
Description:
  zh-cn: 在现有VPC基础上，通过ROS模板部署Spark集群，含管理节点与弹性伸缩节点。配置Java 8, Hadoop 2.7.7, Scala 2.12.1,
    Spark 2.1.0环境，并设置安全组8080端口以访问管理界面。
  en: On top of the existing VPC infrastructure, deploy a Spark cluster using ROS
    (Resource Orchestration Service) templates, comprising both management nodes and
    auto-scaling nodes. Configure the environment with Java 8, Hadoop 2.7.7, Scala
    2.12.1, and Spark 2.1.0. Additionally, configure the security group to open port
    8080 for accessing the management interface.
Parameters:
  VpcId:
    Type: String
    Label:
      en: Existing VPC Instance ID
      zh-cn: 现有VPC的实例ID
    Description:
      en: Please search the ID starting with (vpc-xxx)from console-Virtual Private
        Cloud
      zh-cn: 控制台-VPC-专有网络下查询
    AssociationProperty: ALIYUN::ECS::VPC::VPCId
  VSwitchZoneId:
    Type: String
    Label:
      en: VSwitch Zone ID
      zh-cn: 交换机可用区
    Description:
      en: New Switch Availability Zone ID
      zh-cn: 新建交换机Switch的可用区ID
    AssociationProperty: ALIYUN::ECS::Instance::ZoneId
  VSwitchId:
    Type: String
    Label:
      en: VSwitch ID
      zh-cn: 网络交换机ID
    Description:
      en: Please search the business VSwitch ID starting with(vsw-xxx)from console-Virtual
        Private Cloud-VSwitches
      zh-cn: 现有业务网络交换机的实例ID,控制台-VPC-专有网络-交换机下查询
    AssociationProperty: ALIYUN::ECS::VSwitch::VSwitchId
    AssociationPropertyMetadata:
      VpcId: VpcId
  SecurityGroupId:
    Type: String
    Label:
      en: Business Security Group ID
      zh-cn: 业务安全组ID
    Description:
      en: Please search the business security group ID starting with(sg-xxx)from console-ECS-Network
        & Security
      zh-cn: 现有业务安全组的实例ID,控制台-ECS-网络与安全-安全组下查询
    AssociationProperty: ALIYUN::ECS::SecurityGroup::SecurityGroupId
    AssociationPropertyMetadata:
      VpcId: VpcId
  InstanceType:
    Type: String
    Label:
      en: Instance Type
      zh-cn: 实例规格
    Description:
      en: <font color='blue'><b>1.Before selecting the model please confirm that the
        current available zone under the model is in stock, some models need to be
        reported in advance</b></font>]<br><font color='blue'><b>2.List of optional
        models</font>]<br></b></font>[ecs.c5.large <font color='green'>2vCPU 4GiB
        Intranet bandwidth1Gbps In-grid sending and receiving packages30MillionPPSS</font>]<br></b>[ecs.c5.xlarge
        <font color='green'>4vCPU 8GiB Intranet bandwidth1.5Gbps In-grid sending and
        receiving packages50MillionPPS</font>]<br></b>[ecs.c5.2xlarge <font color='green'>8vCPU
        16GiB Intranet bandwidth2.5Gbps In-grid sending and receiving packages80MillionPPS</font>]
      zh-cn: <font color='blue'><b>1.选择机型前请先确认当前可用区下该机型是否有货，部分机型需要提前报备</b></font><br><font
        color='blue'><b>2.可选机型列表</font><br></b></font>[ecs.c5.large <font color='green'>2vCPU
        4GiB 内网带宽1Gbps 内网收发包30万PPS</font>]<br></b>[ecs.c5.xlarge <font color='green'>4vCPU
        8GiB 内网带宽1.5Gbps 内网收发包50万PPS</font>]<br></b>[ecs.c5.2xlarge <font color='green'>8vCPU
        16GiB 内网带宽2.5Gbps 内网收发包80万PPS</font>]
    AssociationProperty: ALIYUN::ECS::Instance::InstanceType
    AssociationPropertyMetadata:
      ZoneId: VSwitchZoneId
  InstancePassword:
    Type: String
    Label:
      en: Instance Password
      zh-cn: 实例密码
    Description:
      en: Server login password, Length 8-30, must contain three(Capital letters,
        lowercase letters, numbers, ()`~!@#$%^&*_-+=|{}[]:;'<>,.?/ Special symbol
        in)
      zh-cn: 服务器登录密码,长度8-30，必须包含三项（大写字母、小写字母、数字、 ()`~!@#$%^&*_-+=|{}[]:;'<>,.?/ 中的特殊符号）
    ConstraintDescription:
      en: Length 8-30, must contain three(Capital letters, lowercase letters, numbers,
        ()`~!@#$%^&*_-+=|{}[]:;'<>,.?/ Special symbol in).
      zh-cn: 长度8-30，必须包含三项（大写字母、小写字母、数字、 ()`~!@#$%^&*_-+=|{}[]:;'<>,.?/ 中的特殊符号）。
    AllowedPattern: '[0-9A-Za-z\_\-\&:;''<>,=%`~!@#\(\)\$\^\*\+\|\{\}\[\]\.\?\/]+$'
    MinLength: 8
    MaxLength: 30
    NoEcho: true
  DiskCategory:
    Type: String
    Label:
      en: Disk Type
      zh-cn: 磁盘类型
    Description:
      en: '<font color=''blue''><b>Optional values:</b></font><br>[cloud_efficiency:
        <font color=''green''>Efficient Cloud Disk</font>]<br>[cloud_ssd: <font color=''green''>SSD
        Cloud Disk</font>]'
      zh-cn: '<font color=''blue''><b>可选值：</b></font><br>[cloud_efficiency: <font
        color=''green''>高效云盘</font>]<br>[cloud_ssd: <font color=''green''>SSD云盘</font>]'
    Default: cloud_efficiency
    AllowedValues:
    - cloud_efficiency
    - cloud_ssd
  DiskSize:
    Type: Number
    Label:
      en: System Disk Space
      zh-cn: 系统盘空间
    Description:
      en: ''
      zh-cn: 实例系统盘大小，单位为GiB。取值范围：20~500
    Default: 40
    MinValue: 20
    MaxValue: 500
  Amount:
    Type: Number
    Label:
      en: Instance Amount
      zh-cn: 实例数量
    Description:
      en: 'ECS Instance Amount, Allowed value: 3~10'
      zh-cn: 购买实例数量，允许值：3~10
    Default: 3
    MinValue: 3
    MaxValue: 10
  InstanceImageId:
    Type: String
    Label:
      en: Image ID
      zh-cn: 镜像ID
    Description:
      en: Image ID，See detail：<b><a href='https://www.alibabacloud.com/help/doc-detail/112977.html'
        target='_blank'><font color='blue'>Find the mirror</font></a></b>
      zh-cn: 镜像ID, 详见：<b><a href='https://help.aliyun.com/document_detail/112977.html'
        target='_blank'><font color='blue'>查找镜像</font></a></b>
    Default: centos_7
Resources:
  RamRole:
    Type: ALIYUN::RAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Action: sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - oos.aliyuncs.com
        Version: '1'
      Policies:
      - PolicyDocument:
          Statement:
          - Action:
            - ecs:*
            Effect: Allow
            Resource:
            - '*'
          - Action:
            - vpc:DescribeVpcs
            - vpc:DescribeVSwitches
            Effect: Allow
            Resource:
            - '*'
          - Action:
            - ess:CompleteLifecycleAction
            Effect: Allow
            Resource:
            - '*'
          Version: '1'
        PolicyName:
          Fn::Join:
          - ''
          - - StackId-
            - Ref: ALIYUN::StackId
      RoleName:
        Fn::Join:
        - ''
        - - StackId-
          - Ref: ALIYUN::StackId
    Metadata:
      ALIYUN::ROS::Designer:
        id: c40eb24b-0042-4ad8-b85c-b6155fa52238
  OOSTemplateIn:
    Type: ALIYUN::OOS::Template
    Properties:
      Content:
        Fn::Join:
        - ''
        - - '{"FormatVersion": "OOS-2019-06-01","Parameters": {"regionId": {"Type":
            "String","Default": "'
          - Ref: ALIYUN::Region
          - '"},"instanceIds": {"Type": "List","Default": ["${instanceId}"]},"lifecycleHookId":
            {"Type": "String","Default": "${lifecycleHookId}"},"lifecycleActionToken":
            {"Type": "String","Default": "${lifecycleActionToken}"}},"RamRole": "'
          - Fn::GetAtt:
            - RamRole
            - RoleName
          - '","Tasks": [{"Name": "runCommand","Action": "ACS::ECS::RunCommand","OnError":
            "CompleteLifecycleActionForAbandon","OnSuccess": "CompleteLifecycleActionForContinue","Properties":
            {"regionId": "{{ regionId }}","commandContent": "'
          - 'cd /software && rm -f /etc/hosts && rm -f /software/spark/conf/slaves
            && bash /software/spark/sbin/stop-slave.sh && bash rm_nodes.sh '
          - Ref: Amount
          - ' && sleep 15'
          - '","instanceId": "{{ ACS::TaskLoopItem }}","commandType": "RunShellScript"},
            "Loop": {"RateControl": {"Mode":"Concurrency","MaxErrors":0,"Concurrency":10},"Items":
            "{{ instanceIds }}","Outputs": {"commandOutputs": {"AggregateType": "Fn::ListJoin","AggregateField":
            "commandOutput"}}},"Outputs": {"commandOutput": {"Type": "String","ValueSelector":
            "invocationOutput"}}}, {"Name": "CompleteLifecycleActionForContinue","Action":
            "ACS::ExecuteAPI","OnSuccess": "ACS::END","Properties": {"Service": "ESS","API":
            "CompleteLifecycleAction","Parameters": {"RegionId": "{{ regionId }}","LifecycleHookId":
            "{{ lifecycleHookId }}","LifecycleActionToken": "{{ lifecycleActionToken
            }}"}}}, {"Name": "CompleteLifecycleActionForAbandon","Action": "ACS::ExecuteAPI","Properties":
            {"Service": "ESS","API": "CompleteLifecycleAction","Parameters": {"RegionId":
            "{{ regionId }}","LifecycleHookId": "{{ lifecycleHookId }}","LifecycleActionToken":
            "{{ lifecycleActionToken }}","LifecycleActionResult": "ABANDON"}}}]}'
      TemplateName:
        Fn::Join:
        - ''
        - - StackId-
          - Ref: ALIYUN::StackId
          - -In
    DependsOn:
    - RamRole
    Metadata:
      ALIYUN::ROS::Designer:
        id: f38f57e5-4fad-404f-9550-45331dca1d60
  EcsInstanceGroupMaster:
    Type: ALIYUN::ECS::InstanceGroup
    Properties:
      ZoneId:
        Ref: VSwitchZoneId
      VpcId:
        Ref: VpcId
      VSwitchId:
        Ref: VSwitchId
      SecurityGroupId:
        Ref: SecurityGroupId
      ImageId:
        Ref: InstanceImageId
      AllocatePublicIP: true
      HostName:
        Fn::Join:
        - ''
        - - Spark
          - -[0,3]
      InstanceChargeType: PostPaid
      InstanceName:
        Fn::Join:
        - ''
        - - Spark
          - -[0,3]
      InstanceType:
        Ref: InstanceType
      IoOptimized: optimized
      MaxAmount: 1
      Password:
        Ref: InstancePassword
      SystemDiskCategory:
        Ref: DiskCategory
      UserData:
        Fn::Replace:
        - ros-notify:
            Fn::GetAtt:
            - RosWaitConditionMasterHandle
            - CurlCli
        - Fn::Join:
          - ''
          - - "#!/bin/sh \n"
            - PASSWORD="
            - Ref: InstancePassword
            - "\" \n"
            - NODE_COUNT=
            - Ref: Amount
            - " \n"
            - ROS_NOTIFY="
            - Fn::GetAtt:
              - RosWaitConditionClusterHandle
              - CurlCli
            - "\" \n"
            - "sleep 10 \n"
            - "set -e \n"
            - "HOST_IP=$(ifconfig eth0 | awk '/inet /{print $2}') \n"
            - "HOST_NAME=$(hostname) \n"
            - "OSS_NAME=\"ros-template-resources\" \n"
            - "OSS_REGION=\"cn-beijing\" \n"
            - "ENDPOINT=\"aliyuncs.com\" \n"
            - "ENV_DIR=\"/software\" \n"
            - "RESOURCE_DIR=\"${ENV_DIR}/resources\" \n"
            - "BASH_PATH=\"/etc/profile\" \n"
            - "JDK_NAME=\"jdk-8u251-linux-i586\" \n"
            - "JDK_RPM=\"${JDK_NAME}.rpm\" \n"
            - "HADOOP=\"hadoop\" \n"
            - "HADOOP_EXTRACT_NAME=\"${HADOOP}-2.7.7\" \n"
            - "HADOOP_GZ=\"${HADOOP_EXTRACT_NAME}.tar.gz\" \n"
            - "SPARK=\"spark\" \n"
            - "SPARK_EXTRACT_NAME=\"${SPARK}-2.1.0-bin-hadoop2.7\" \n"
            - "SPARK_TGZ=\"${SPARK_EXTRACT_NAME}.tgz\" \n"
            - "SCALA=\"scala\" \n"
            - "SCALA_EXTRACT_NAME=\"${SCALA}-2.12.1\" \n"
            - "SCALA_TGZ=\"${SCALA_EXTRACT_NAME}.tgz\" \n"
            - "objectList=(\"JDK/${JDK_RPM}\" \"Hadoop/${SPARK_TGZ}\" \"Hadoop/${HADOOP_GZ}\"\
              \ \"Hadoop/${SCALA_TGZ}\") \n"
            - " \n"
            - "LOG_FILE=\"${ENV_DIR}/userdata.log\" \n"
            - "CLUSTER_FILE=\"${ENV_DIR}/cluster.sh\" \n"
            - "SSH_SCRIPT_FILE=\"${ENV_DIR}/ssh.sh\" \n"
            - "RM_NODES_FILE=\"${ENV_DIR}/rm_nodes.ini\" \n"
            - "ADD_NODES_FILE=\"${ENV_DIR}/add_nodes.ini\" \n"
            - "NODES_INFO_FILE=\"${ENV_DIR}/nodes_info.ini\" \n"
            - " \n"
            - "HADOOP_HOME=${ENV_DIR}/hadoop \n"
            - " \n"
            - "recordLog() { \n"
            - "    time=$(date \"+%Y-%m-%d %H:%M:%S\") \n"
            - "    echo \"$time --- $1\" >>${LOG_FILE} \n"
            - "} \n"
            - " \n"
            - "createDir() { \n"
            - "    dir=$1 \n"
            - "    if [ -d \"${dir}\" ]; then \n"
            - "        recordLog \"Create failed, dir-${dir} is existed\" \n"
            - "    else \n"
            - "        mkdir -p \"${dir}\" \n"
            - "        recordLog \"Create Dir-${dir} successful\" \n"
            - "    fi \n"
            - "} \n"
            - " \n"
            - "prepareEnv() { \n"
            - "    mkdir ${ENV_DIR} \n"
            - "    createDir ${RESOURCE_DIR} \n"
            - "} \n"
            - " \n"
            - "download() { \n"
            - "    objectName=$1 \n"
            - "    wget https://${OSS_NAME}.oss-${OSS_REGION}.${ENDPOINT}/${objectName}\
              \ -P ${RESOURCE_DIR} \n"
            - "    recordLog \"Download ${objectName} successful\" \n"
            - "} \n"
            - " \n"
            - "downloadResources() { \n"
            - "    for object in ${objectList[@]}; do \n"
            - "        download ${object} \n"
            - "    done \n"
            - "    recordLog \"Download all resources successful\" \n"
            - "} \n"
            - " \n"
            - "installJavaAndConfig() { \n"
            - "    yum -y install glibc.i686 \n"
            - "    rpm -Uvh ${RESOURCE_DIR}/${JDK_RPM} \n"
            - "    # config \n"
            - "    export JAVA_HOME=$(find / -name jdk1.8.0_*) \n"
            - "    echo \"export JAVA_HOME=${JAVA_HOME}\" >>${BASH_PATH} \n"
            - "    echo \"export JRE_HOME=${JAVA_HOME}/jre\" >>${BASH_PATH} \n"
            - "    echo \"export CLASSPATH=.:${JAVA_HOME}/lib:${JAVA_HOME}/jre/lib\"\
              \ >>${BASH_PATH} \n"
            - "    echo \"export PATH=${JAVA_HOME}/bin:$PATH\" >>${BASH_PATH} \n"
            - "    source ${BASH_PATH} \n"
            - "    recordLog \"Config java successful\" \n"
            - "} \n"
            - " \n"
            - "generateScript() { \n"
            - "    ssh-keygen -t rsa -P '' -f '/root/.ssh/id_rsa' \n"
            - "    yum -y install expect \n"
            - "    echo '#!/bin/bash' >${SSH_SCRIPT_FILE} \n"
            - "    echo 'name_or_ip=$1' >>${SSH_SCRIPT_FILE} \n"
            - "    echo 'authorized_key=$2' >>${SSH_SCRIPT_FILE} \n"
            - "    echo 'expect <<EOF' >>${SSH_SCRIPT_FILE} \n"
            - "    echo 'set timeout 150' >>${SSH_SCRIPT_FILE} \n"
            - "    echo \"spawn ssh root@\\${name_or_ip} echo \\\"\\${authorized_key}\\\
              \" >> /root/.ssh/authorized_keys\" >>${SSH_SCRIPT_FILE} \n"
            - "    echo 'expect {' >>${SSH_SCRIPT_FILE} \n"
            - "    echo \"  \\\"*yes/no*\\\" { send \\\"yes\\n\\\"; exp_continue }\"\
              \ >>${SSH_SCRIPT_FILE} \n"
            - "    echo \"  \\\"*password:\\\" { send \\\"${PASSWORD}\\n\\\" }\" >>${SSH_SCRIPT_FILE}\
              \ \n"
            - "    echo '}' >>${SSH_SCRIPT_FILE} \n"
            - "    echo 'expect eof' >>${SSH_SCRIPT_FILE} \n"
            - "    echo 'EOF' >>${SSH_SCRIPT_FILE} \n"
            - "    chmod +x ${SSH_SCRIPT_FILE} \n"
            - "    recordLog \"Generate ${SSH_SCRIPT_FILE} successful\" \n"
            - "    echo '#!/bin/bash' >\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo 'host_ip=$1' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo 'expect <<EOF' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo 'set timeout 150' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo \"spawn ssh root@\\${host_ip} exit;\" >>\"${SSH_SCRIPT_FILE}.login\"\
              \ \n"
            - "    echo 'expect {' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo \"  \\\"*yes/no*\\\" { send \\\"yes\\n\\\" }\" >>\"${SSH_SCRIPT_FILE}.login\"\
              \ \n"
            - "    echo '}' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo 'expect eof' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo 'EOF' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    chmod +x \"${SSH_SCRIPT_FILE}.login\" \n"
            - "    recordLog \"Generate ${SSH_SCRIPT_FILE}.login successful\" \n"
            - "} \n"
            - " \n"
            - "configLocalSSH() { \n"
            - "    authorized_key=$(cat /root/.ssh/id_rsa.pub) \n"
            - "    bash ${SSH_SCRIPT_FILE} \"${HOST_IP}\" \"${authorized_key}\" \n"
            - "    bash \"${SSH_SCRIPT_FILE}.login\" \"0.0.0.0\" \n"
            - "    bash \"${SSH_SCRIPT_FILE}.login\" \"localhost\" \n"
            - "    recordLog \"Config expect-localhost successful\" \n"
            - "} \n"
            - " \n"
            - "installAndConfigHadoop(){ \n"
            - "    # download \n"
            - "    tar -zxvf ${RESOURCE_DIR}/${HADOOP_GZ} -C ${ENV_DIR} && cd ${ENV_DIR}\
              \ && mv ${HADOOP_EXTRACT_NAME} ${HADOOP} \n"
            - "    echo \"export HADOOP_HOME=${HADOOP_HOME}\" >> ${BASH_PATH} \n"
            - "    echo \"export PATH=${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH\"\
              \ >> ${BASH_PATH} \n"
            - "    source ${BASH_PATH} \n"
            - "    # config core-site.xml \n"
            - "    mv ${HADOOP_HOME}/etc/hadoop/core-site.xml ${HADOOP_HOME}/etc/hadoop/bak.core-site.xml\
              \ \n"
            - '    cat >${HADOOP_HOME}/etc/hadoop/core-site.xml <<EOF

              '
            - "<configuration> \n"
            - "  <property> \n"
            - "    <name>fs.defaultFS</name> \n"
            - "    <value>hdfs://${HOST_IP}:9000</value> \n"
            - "  </property> \n"
            - "  <property> \n"
            - "    <name>hadoop.tmp.dir</name> \n"
            - "    <value>${ENV_DIR}/tmp</value> \n"
            - "  </property> \n"
            - "</configuration> \n"
            - 'EOF

              '
            - "    # hdfs-site.xml \n"
            - "    mv ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml ${HADOOP_HOME}/etc/hadoop/bak.hdfs-site.xml\
              \ \n"
            - '    cat >${HADOOP_HOME}/etc/hadoop/hdfs-site.xml <<EOF

              '
            - "<configuration> \n"
            - "  <property> \n"
            - "    <name>dfs.replication</name> \n"
            - "    <value>${NODE_COUNT}</value> \n"
            - "  </property> \n"
            - "</configuration> \n"
            - 'EOF

              '
            - "    # config mapred-site.xml \n"
            - '    cat >${HADOOP_HOME}/etc/hadoop/mapred-site.xml <<EOF

              '
            - "<configuration> \n"
            - "  <property> \n"
            - "    <name>mapreduce.framework.name</name> \n"
            - "    <value>yarn</value> \n"
            - "  </property> \n"
            - "</configuration> \n"
            - 'EOF

              '
            - "    # config yarn-site.xml \n"
            - "    mv ${HADOOP_HOME}/etc/hadoop/yarn-site.xml ${HADOOP_HOME}/etc/hadoop/bak.yarn-site.xml\
              \ \n"
            - '    cat >${HADOOP_HOME}/etc/hadoop/yarn-site.xml <<EOF

              '
            - "<configuration> \n"
            - "  <property> \n"
            - "    <name>yarn.nodemanager.aux-services</name> \n"
            - "    <value>mapreduce_shuffle</value> \n"
            - "  </property> \n"
            - "  <property> \n"
            - "    <name>yarn.resourcemanager.hostname</name> \n"
            - "    <value>${HOST_IP}</value> \n"
            - "  </property> \n"
            - "</configuration> \n"
            - 'EOF

              '
            - "    # config java home \n"
            - "    sed -i 's/export JAVA_HOME=/#export JAVA_HOME=/' ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh\
              \ \n"
            - "    sed -i \"/#export JAVA_HOME=/a export JAVA_HOME=${JAVA_HOME}\"\
              \ ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh \n"
            - "    sed -i 's/export JAVA_HOME=/#export JAVA_HOME=/' ${HADOOP_HOME}/etc/hadoop/mapred-env.sh\
              \ \n"
            - "    sed -i \"/#export JAVA_HOME=/a export JAVA_HOME=${JAVA_HOME}\"\
              \ ${HADOOP_HOME}/etc/hadoop/mapred-env.sh \n"
            - "    sed -i 's/export JAVA_HOME=/#export JAVA_HOME=/' ${HADOOP_HOME}/etc/hadoop/yarn-env.sh\
              \ \n"
            - "    sed -i \"/#export JAVA_HOME=/a export JAVA_HOME=${JAVA_HOME}\"\
              \ ${HADOOP_HOME}/etc/hadoop/yarn-env.sh \n"
            - "    recordLog \"Config hadoop successful\" \n"
            - "} \n"
            - " \n"
            - "installSparkAndConfig() { \n"
            - "   tar zxvf ${RESOURCE_DIR}/spark-*.tgz -C ${ENV_DIR} && cd ${ENV_DIR}\
              \ && mv ${SPARK_EXTRACT_NAME} ${SPARK} \n"
            - "   mv ${ENV_DIR}/${SPARK}/conf/spark-env.sh.template ${ENV_DIR}/${SPARK}/conf/spark-env.sh\
              \ \n"
            - "   echo \"export SCALA_HOME=${ENV_DIR}/scala\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
              \ \n"
            - "   echo \"export JAVA_HOME=${JAVA_HOME}\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
              \ \n"
            - "   echo \"export HADOOP_HOME=${ENV_DIR}/hadoop\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
              \ \n"
            - "   echo \"export HADOOP_CONF_DIR=${ENV_DIR}/hadoop/etc/hadoop\" >>\
              \ ${ENV_DIR}/spark/conf/spark-env.sh \n"
            - "   echo \"export SPARK_HOME=${ENV_DIR}/spark\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
              \ \n"
            - "   echo \"export SPARK_MASTER_IP=${HOST_IP}\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
              \ \n"
            - "   echo \"export SPARK_EXECUTOR_MEMORY=1G\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
              \ \n"
            - "   recordLog \"Config spark successful\" \n"
            - "} \n"
            - " \n"
            - "installScalaAndConfig() { \n"
            - "    tar zxvf ${RESOURCE_DIR}/${SCALA_TGZ} -C ${ENV_DIR} && cd ${ENV_DIR}\
              \ && mv ${SCALA_EXTRACT_NAME} ${SCALA} \n"
            - "    echo \"export SCALA_HOME=${ENV_DIR}/scala\" >> ${BASH_PATH} \n"
            - "    echo \"PATH=$PATH:${ENV_DIR}/scala/bin\" >> ${BASH_PATH} \n"
            - "    echo \"export JAVA_HOME=${JAVA_HOME}\" >> ${BASH_PATH} \n"
            - "    echo \"export JRE_HOME=${JAVA_HOME}/jre\" >> ${BASH_PATH} \n"
            - "    echo \"export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\" >>\
              \ ${BASH_PATH} \n"
            - "    echo \"export PATH=${JAVA_HOME}/bin:$PATH\" >> ${BASH_PATH} \n"
            - "    echo \"export HADOOP_HOME=${ENV_DIR}/hadoop\" >> ${BASH_PATH} \n"
            - "    echo \"export PATH=.:${ENV_DIR}/hadoop/bin:$PATH\" >> ${BASH_PATH}\
              \ \n"
            - "    echo \"export SPARK_HOME=${ENV_DIR}/spark\" >> ${BASH_PATH} \n"
            - "    echo \"export PATH=.:${ENV_DIR}/spark/bin:$PATH\" >> ${BASH_PATH}\
              \ \n"
            - "    source ${BASH_PATH} \n"
            - "    recordLog \"Config scala successful\" \n"
            - "} \n"
            - " \n"
            - " \n"
            - "generateClusterScript() { \n"
            - '    cat >${CLUSTER_FILE} <<EOF

              '
            - "#!/bin/bash \n"
            - "NODE_COUNT=\\$1 \n"
            - "HOST_IP=\\$(ifconfig eth0 | awk '/inet /{print \\$2}') \n"
            - "#sleep 10 \n"
            - "#set -e \n"
            - "ENV_DIR=\"/software\" \n"
            - "HADOOP=\"hadoop\" \n"
            - "SPARK=\"spark\" \n"
            - " \n"
            - "LOG_FILE=\"\\${ENV_DIR}/userdata.log\" \n"
            - "SSH_SCRIPT_FILE=\"\\${ENV_DIR}/ssh.sh\" \n"
            - "RM_NODES_FILE=\"\\${ENV_DIR}/rm_nodes.ini\" \n"
            - "ADD_NODES_FILE=\"\\${ENV_DIR}/add_nodes.ini\" \n"
            - "NODES_INFO_FILE=\"\\${ENV_DIR}/nodes_info.ini\" \n"
            - "NODES_COUNT_FILE=\"\\${ENV_DIR}/nodes_count.ini\" \n"
            - "HOSTS_PATH=\"/etc/hosts\" \n"
            - " \n"
            - "HADOOP_HOME=\\${ENV_DIR}/\\${HADOOP} \n"
            - "SPARK_HOME=\\${ENV_DIR}/\\${SPARK} \n"
            - " \n"
            - "configCluster() { \n"
            - "    if [ ! -f \\${NODES_COUNT_FILE} ]; then \n"
            - "        echo \"正常创建config\" >> \\${LOG_FILE} \n"
            - "        nodes_info=\\$(cat \\${NODES_INFO_FILE}) \n"
            - "        for node_info in \\${nodes_info[@]}; do \n"
            - "            new_node_info=(\\${node_info//:/ }) \n"
            - "            node_ip=\\${new_node_info[0]} \n"
            - "            node_hostname=\\${new_node_info[1]} \n"
            - "            echo \"\\${node_ip} \\${node_hostname}\" >>\\${HOSTS_PATH}\
              \ \n"
            - "            echo \"\\${node_hostname}\" >>\"\\${SPARK_HOME}/conf/slaves\"\
              \ \n"
            - "            if [[ \"\\${HOST_IP}\" == \"\\${node_ip}\" ]]; then \n"
            - "                continue \n"
            - "            fi \n"
            - "            authorized_key=\\$(cat /root/.ssh/id_rsa.pub) \n"
            - "            bash \\${SSH_SCRIPT_FILE} \"\\${node_hostname}\" \"\\${authorized_key}\"\
              \ \n"
            - "        done \n"
            - "        for node_info in \\${nodes_info[@]}; do \n"
            - "            new_node_info=(\\${node_info//:/ }) \n"
            - "            node_ip=\\${new_node_info[0]} \n"
            - "            node_hostname=\\${new_node_info[1]} \n"
            - "            if [[ \"\\${HOST_IP}\" == \"\\${node_ip}\" ]]; then \n"
            - "                continue \n"
            - "            fi \n"
            - "            scp -r \\${HOSTS_PATH} \\${node_hostname}:/etc \n"
            - "            scp -r \\${SPARK_HOME}/conf/slaves \\${node_hostname}:\\\
              ${SPARK_HOME}/conf \n"
            - "        done \n"
            - "    else \n"
            - "        old_node_count=\\$(cat \\${NODES_COUNT_FILE}) \n"
            - "        if [ \\${NODE_COUNT} -gt \\${old_node_count} ]; then \n"
            - "            nodes_info=\\$(cat \\${NODES_INFO_FILE}) \n"
            - "            for node_info in \\${nodes_info[@]}; do \n"
            - "                new_node_info=(\\${node_info//:/ }) \n"
            - "                node_ip=\\${new_node_info[0]} \n"
            - "                node_hostname=\\${new_node_info[1]} \n"
            - "                grep \"\\${node_hostname}\" \\${HOSTS_PATH} \n"
            - "                if [ \\$? -eq \"0\" ]; then \n"
            - "                    continue \n"
            - "                fi \n"
            - "                echo \"\\${node_ip} \\${node_hostname}\" >>\\${HOSTS_PATH}\
              \ \n"
            - "                echo \"\\${node_hostname}\" >>\"\\${SPARK_HOME}/conf/slaves\"\
              \ \n"
            - "                authorized_key=\\$(cat /root/.ssh/id_rsa.pub) \n"
            - "                bash \\${SSH_SCRIPT_FILE} \"\\${node_hostname}\" \"\
              \\${authorized_key}\" \n"
            - "                sed -i \"s%<value>\\${old_node_count}</value>%<value>\\\
              ${NODE_COUNT}</value>%g\" \\${HADOOP_HOME}/etc/hadoop/hdfs-site.xml\
              \ \n"
            - "            done \n"
            - "            for node_info in \\${nodes_info[@]}; do \n"
            - "                new_node_info=(\\${node_info//:/ }) \n"
            - "                node_ip=\\${new_node_info[0]} \n"
            - "                node_hostname=\\${new_node_info[1]} \n"
            - "                if [ \"\\${HOST_IP}\" == \"\\${node_ip}\" ]; then \n"
            - "                    continue \n"
            - "                fi \n"
            - "                scp -r \\${HOSTS_PATH} \\${node_hostname}:/etc \n"
            - "                scp -r \\${SPARK_HOME}/conf/slaves \\${node_hostname}:\\\
              ${SPARK_HOME}/conf \n"
            - "                scp -r \\${HADOOP_HOME}/etc/hadoop/hdfs-site.xml \\\
              ${node_hostname}:\\${HADOOP_HOME}/etc/hadoop \n"
            - "            done \n"
            - "        else \n"
            - "            rm_nodes_info=\\$(cat \\${RM_NODES_FILE}) \n"
            - "            for rm_node in \\${rm_nodes_info[@]}; do \n"
            - "                rm_node_info=(\\${rm_node//:/ }) \n"
            - "                node_ip=\\${rm_node_info[0]} \n"
            - "                node_hostname=\\${rm_node_info[1]} \n"
            - "                grep \"\\${node_hostname}\" \"\\${HOSTS_PATH}\" \n"
            - "                if [ \\$? -eq \"0\" ]; then \n"
            - "                    sed -i \"/\\${node_hostname}/d\" \\${HOSTS_PATH}\
              \ \n"
            - "                    sed -i \"/\\${node_hostname}/d\" \\${SPARK_HOME}/conf/slaves\
              \ \n"
            - "                    sed -i \"s%<value>\\${old_node_count}</value>%<value>\\\
              ${NODE_COUNT}</value>%g\" \\${HADOOP_HOME}/etc/hadoop/hdfs-site.xml\
              \ \n"
            - "                fi \n"
            - "            done \n"
            - "            nodes_info=\\$(cat \\${NODES_INFO_FILE}) \n"
            - "            for node_info in \\${nodes_info[@]}; do \n"
            - "                new_node_info=(\\${node_info//:/ }) \n"
            - "                node_ip=\\${new_node_info[0]} \n"
            - "                node_hostname=\\${new_node_info[1]} \n"
            - "                if [ \"\\${HOST_IP}\" == \"\\${node_ip}\" ]; then \n"
            - "                    continue \n"
            - "                fi \n"
            - "                scp -r \\${HOSTS_PATH} \\${node_hostname}:/etc \n"
            - "                scp -r \\${SPARK_HOME}/conf/slaves \\${node_hostname}:\\\
              ${SPARK_HOME}/conf \n"
            - "                scp -r \\${HADOOP_HOME}/etc/hadoop/hdfs-site.xml \\\
              ${node_hostname}:\\${HADOOP_HOME}/etc/hadoop \n"
            - "            done \n"
            - "        fi \n"
            - "    fi \n"
            - "} \n"
            - " \n"
            - "startCluster() { \n"
            - "    if [ ! -f \\${NODES_COUNT_FILE} ]; then \n"
            - "        echo \"正常创建start\" >> \\${LOG_FILE} \n"
            - "        \\${HADOOP_HOME}/bin/hdfs namenode -format \n"
            - "        \\${HADOOP_HOME}/sbin/start-dfs.sh \n"
            - "        \\${HADOOP_HOME}/sbin/start-yarn.sh \n"
            - "        nodes_info=\\$(cat \\${NODES_INFO_FILE}) \n"
            - "        for node_info in \\${nodes_info[@]}; do \n"
            - "            new_node_info=(\\${node_info//:/ }) \n"
            - "            node_ip=\\${new_node_info[0]} \n"
            - "            node_hostname=\\${new_node_info[1]} \n"
            - "            if [[ \"\\${HOST_IP}\" == \"\\${node_ip}\" ]]; then \n"
            - "                continue \n"
            - "            fi \n"
            - "            ssh root@\\${node_hostname} \"bash \\${ENV_DIR}/hadoop/sbin/hadoop-daemon.sh\
              \ start datanode && bash \\${ENV_DIR}/hadoop/sbin/yarn-daemon.sh start\
              \ nodemanager && exit;\" \n"
            - "        done \n"
            - "        echo \"Start hadoop successful\" >>\\${LOG_FILE} \n"
            - "        bash \\${ENV_DIR}/spark/sbin/start-all.sh \n"
            - "        echo \"Start spark successful\" >>\\${LOG_FILE} \n"
            - "    else \n"
            - "        old_node_count=\\$(cat \\${NODES_COUNT_FILE}) \n"
            - "        if [ \\${NODE_COUNT} -gt \\${old_node_count} ]; then \n"
            - "            nodes_info=\\$(cat \\${NODES_INFO_FILE}) \n"
            - "            for node_info in \\${nodes_info[@]}; do \n"
            - "                new_node_info=(\\${node_info//:/ }) \n"
            - "                node_ip=\\${new_node_info[0]} \n"
            - "                node_hostname=\\${new_node_info[1]} \n"
            - "                grep \"\\${node_hostname}\" \\${HOSTS_PATH} \n"
            - "                if [ \\$? -eq \"0\" ]; then \n"
            - "                    continue \n"
            - "                fi \n"
            - "                ssh root@\\${node_hostname} \"bash \\${ENV_DIR}/hadoop/sbin/hadoop-daemon.sh\
              \ start datanode && bash \\${ENV_DIR}/hadoop/sbin/yarn-daemon.sh start\
              \ nodemanager && exit;\" \n"
            - "            done \n"
            - "            echo \"Start hadoop(scale-out) successful\" >>\\${LOG_FILE}\
              \ \n"
            - "            bash \\${ENV_DIR}/spark/sbin/stop-all.sh \n"
            - "            sleep 3 \n"
            - "            bash \\${ENV_DIR}/spark/sbin/start-all.sh \n"
            - "            echo \"Start spark(scale-out) successful\" >>\\${LOG_FILE}\
              \ \n"
            - "            rm -f \\${ADD_NODES_FILE} \n"
            - "        else \n"
            - "            nodes_info=\\$(cat \\${NODES_INFO_FILE}) \n"
            - "            echo \"Start hadoop(scale-in) successful\" >>\\${LOG_FILE}\
              \ \n"
            - "            bash \\${ENV_DIR}/spark/sbin/stop-all.sh \n"
            - "            sleep 10 \n"
            - "            bash \\${ENV_DIR}/spark/sbin/start-all.sh \n"
            - "            echo \"Start spark(scale-in) successful\" >>\\${LOG_FILE}\
              \ \n"
            - "            rm -f \\${RM_NODES_FILE} && touch \\${RM_NODES_FILE} \n"
            - "        fi \n"
            - "    fi \n"
            - "    echo \\${NODE_COUNT} >\\${NODES_COUNT_FILE} \n"
            - "} \n"
            - " \n"
            - "main() { \n"
            - "    nodes_count=\\$(cat \\${NODES_INFO_FILE} | wc -l) \n"
            - "    if [[ \"\\${nodes_count}\" == \"\\${NODE_COUNT}\" ]]; then \n"
            - "        echo \"config cluster ......\" >>\\${LOG_FILE} \n"
            - "        configCluster \n"
            - "        echo \"config finished ......\" >>\\${LOG_FILE} \n"
            - "        echo \"start cluster ......\" >>\\${LOG_FILE} \n"
            - "        startCluster \n"
            - "        echo \"start finished\" >>\\${LOG_FILE} \n"
            - "        ${ROS_NOTIFY} \n"
            - "    fi \n"
            - "} \n"
            - " \n"
            - "main \n"
            - 'EOF

              '
            - "} \n"
            - " \n"
            - "main() { \n"
            - "    prepareEnv \n"
            - "    downloadResources \n"
            - "    generateScript \n"
            - "    configLocalSSH \n"
            - "    installJavaAndConfig \n"
            - "    installAndConfigHadoop \n"
            - "    installSparkAndConfig \n"
            - "    generateClusterScript \n"
            - "    echo \"${HOST_IP}:${HOST_NAME}\" >>${NODES_INFO_FILE} \n"
            - "    touch ${RM_NODES_FILE} \n"
            - "    rm -rf /etc/hosts && touch /etc/hosts \n"
            - "} \n"
            - " \n"
            - "main \n"
            - "ros-notify -d \"{\\\"Status\\\" : \\\"Success\\\"}\" \n"
    DependsOn:
    - OOSTemplateIn
    Metadata:
      ALIYUN::ROS::Designer:
        id: 4d17593b-029a-45bf-849a-38376b2ac955
  OOSTemplateOut:
    Type: ALIYUN::OOS::Template
    Properties:
      Content:
        Fn::Join:
        - ''
        - - "FormatVersion: OOS-2019-06-01\nParameters:\n  regionId:\n    Type: String\n\
            \    Default: "
          - Ref: ALIYUN::Region
          - "\n  instanceIds:\n    Type: List\n    Default:\n      - '${instanceId}'\n\
            \  lifecycleHookId:\n    Type: String\n    Default: '${lifecycleHookId}'\n\
            \  lifecycleActionToken:\n    Type: String\n    Default: '${lifecycleActionToken}'\n\
            RamRole: "
          - Fn::GetAtt:
            - RamRole
            - RoleName
          - "\nTasks:\n  - Name: runCommand\n    Action: 'ACS::ECS::RunCommand'\n\
            \    OnError: CompleteLifecycleActionForAbandon\n    OnSuccess: CompleteLifecycleActionForContinue\n\
            \    Properties:\n      regionId: '{{ regionId }}'\n      commandContent:\
            \ |- \n"
          - Fn::Replace:
            - ros-notify:
                Fn::GetAtt:
                - RosWaitConditionHandleEss
                - CurlCli
            - Fn::Join:
              - ''
              - - "        #!/bin/sh \n"
                - "        hostname=$(hostname) \n"
                - '        MASTER_IP='
                - Fn::Select:
                  - '0'
                  - Fn::GetAtt:
                    - EcsInstanceGroupMaster
                    - PrivateIps
                - '

                  '
                - '        PASSWORD='
                - Ref: InstancePassword
                - '

                  '
                - '        NODE_COUNT='
                - Ref: Amount
                - '

                  '
                - "        echo root:${PASSWORD} | chpasswd \n"
                - "        # open sshd PasswordAuthentication \n"
                - "        sed -i 's/PasswordAuthentication no/PasswordAuthentication\
                  \ yes/g' \"/etc/ssh/sshd_config\" \n"
                - "        service sshd restart \n"
                - '        NODE_COUNT='
                - Ref: Amount
                - '

                  '
                - '        MASTER_HOSTNAME='
                - Fn::Select:
                  - '0'
                  - Fn::GetAtt:
                    - EcsInstanceGroupMaster
                    - HostNames
                - '

                  '
                - "        HOST_IP=$(ifconfig eth0 | awk '/inet /{print $2}') \n"
                - "        HOST_NAME=$(hostname) \n"
                - "        ENV_DIR=\"/software\" \n"
                - "        BASH_PATH=\"/etc/profile\" \n"
                - "        RESOURCE_DIR=\"${ENV_DIR}/resources\" \n"
                - "        set -e \n"
                - "        JDK_NAME=\"jdk-8u251-linux-i586\" \n"
                - "        JDK_RPM=\"${JDK_NAME}.rpm\" \n"
                - "        HADOOP=\"hadoop\" \n"
                - "        HADOOP_EXTRACT_NAME=\"${HADOOP}-2.7.7\" \n"
                - "        HADOOP_GZ=\"${HADOOP_EXTRACT_NAME}.tar.gz\" \n"
                - "        SPARK=\"spark\" \n"
                - "        SPARK_EXTRACT_NAME=\"${SPARK}-2.1.0-bin-hadoop2.7\" \n"
                - "        SPARK_TGZ=\"${SPARK_EXTRACT_NAME}.tgz\" \n"
                - "        SCALA=\"scala\" \n"
                - "        SCALA_EXTRACT_NAME=\"${SCALA}-2.12.1\" \n"
                - "        SCALA_TGZ=\"${SCALA_EXTRACT_NAME}.tgz\" \n"
                - "         \n"
                - "        LOG_FILE=\"${ENV_DIR}/userdata.log\" \n"
                - "        CLUSTER_FILE=\"${ENV_DIR}/cluster.sh\" \n"
                - "        SSH_SCRIPT_FILE=\"${ENV_DIR}/ssh.sh\" \n"
                - "        RM_NODES_FILE=\"${ENV_DIR}/rm_nodes.sh\" \n"
                - "        RM_NODES_INI=\"${ENV_DIR}/rm_nodes.ini\" \n"
                - "        ADD_NODES_FILE=\"${ENV_DIR}/add_nodes.sh\" \n"
                - "        NODES_INFO_FILE=\"${ENV_DIR}/nodes_info.ini\" \n"
                - "         \n"
                - "        HADOOP_HOME=${ENV_DIR}/hadoop \n"
                - "         \n"
                - "        recordLog() { \n"
                - "            time=$(date \"+%Y-%m-%d %H:%M:%S\") \n"
                - "            if [ ! -d ${ENV_DIR} ]; then \n"
                - "                mkdir ${ENV_DIR} \n"
                - "            fi \n"
                - "            echo \"$time --- $1\" >>${LOG_FILE} \n"
                - "        } \n"
                - "          \n"
                - "        createDir() { \n"
                - "            dir=$1 \n"
                - "            if [ -d \"${dir}\" ]; then \n"
                - "                recordLog \"Create failed, dir-${dir} is existed\"\
                  \ \n"
                - "            else \n"
                - "                mkdir -p \"${dir}\" \n"
                - "                recordLog \"Create Dir-${dir} successful\" \n"
                - "            fi \n"
                - "        } \n"
                - "         \n"
                - "        prepareEnv() { \n"
                - "            mkdir ${ENV_DIR} \n"
                - "            createDir ${RESOURCE_DIR} \n"
                - "        } \n"
                - "         \n"
                - "        generateSSHScript() { \n"
                - "            ssh-keygen -t rsa -P '' -f '/root/.ssh/id_rsa' \n"
                - "            yum -y install expect \n"
                - "            echo '#!/bin/bash' >${SSH_SCRIPT_FILE} \n"
                - "            echo 'name_or_ip=$1' >>${SSH_SCRIPT_FILE} \n"
                - "            echo 'authorized_key=$2' >>${SSH_SCRIPT_FILE} \n"
                - "            echo 'expect <<EOF' >>${SSH_SCRIPT_FILE} \n"
                - "            echo 'set timeout 150' >>${SSH_SCRIPT_FILE} \n"
                - "            echo \"spawn ssh root@\\${name_or_ip} echo \\\"\\${authorized_key}\\\
                  \" >> /root/.ssh/authorized_keys\" >>${SSH_SCRIPT_FILE} \n"
                - "            echo 'expect {' >>${SSH_SCRIPT_FILE} \n"
                - "            echo \"  \\\"*yes/no*\\\" { send \\\"yes\\n\\\"; exp_continue\
                  \ }\" >>${SSH_SCRIPT_FILE} \n"
                - "            echo \"  \\\"*password:\\\" { send \\\"${PASSWORD}\\\
                  n\\\" }\" >>${SSH_SCRIPT_FILE} \n"
                - "            echo '}' >>${SSH_SCRIPT_FILE} \n"
                - "            echo 'expect eof' >>${SSH_SCRIPT_FILE} \n"
                - "            echo 'EOF' >>${SSH_SCRIPT_FILE} \n"
                - "            chmod +x ${SSH_SCRIPT_FILE} \n"
                - "            recordLog \"Generate ${SSH_SCRIPT_FILE} successful\"\
                  \ \n"
                - "            echo '#!/bin/bash' >\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            echo 'host_ip=$1' >>\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            echo 'expect <<EOF' >>\"${SSH_SCRIPT_FILE}.login\"\
                  \ \n"
                - "            echo 'set timeout 150' >>\"${SSH_SCRIPT_FILE}.login\"\
                  \ \n"
                - "            echo \"spawn ssh root@\\${host_ip} exit;\" >>\"${SSH_SCRIPT_FILE}.login\"\
                  \ \n"
                - "            echo 'expect {' >>\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            echo \"  \\\"*yes/no*\\\" { send \\\"yes\\n\\\" }\"\
                  \ >>\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            echo '}' >>\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            echo 'expect eof' >>\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            echo 'EOF' >>\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            chmod +x \"${SSH_SCRIPT_FILE}.login\" \n"
                - "            recordLog \"Generate ${SSH_SCRIPT_FILE}.login successful\"\
                  \ \n"
                - "        } \n"
                - "         \n"
                - "        configLocalSSH() { \n"
                - "            authorized_key=$(cat /root/.ssh/id_rsa.pub) \n"
                - "            bash ${SSH_SCRIPT_FILE} \"${MASTER_IP}\" \"${authorized_key}\"\
                  \ \n"
                - "            bash ${SSH_SCRIPT_FILE} \"${HOST_IP}\" \"${authorized_key}\"\
                  \ \n"
                - "            bash \"${SSH_SCRIPT_FILE}.login\" \"0.0.0.0\" \n"
                - "            bash \"${SSH_SCRIPT_FILE}.login\" \"localhost\" \n"
                - "            sed -i \"s/${MASTER_IP}/${MASTER_HOSTNAME},${MASTER_IP}/\"\
                  \ \"/root/.ssh/known_hosts\" \n"
                - "            recordLog \"Config expect-localhost successful\" \n"
                - "        } \n"
                - "         \n"
                - "          \n"
                - "        scpResources() { \n"
                - "            scp -r root@${MASTER_IP}:${RESOURCE_DIR}/* ${RESOURCE_DIR}\
                  \ \n"
                - "            recordLog \"Scp resources successful\" \n"
                - "        } \n"
                - "         \n"
                - "        installJavaAndConfig() { \n"
                - "            yum -y install glibc.i686 \n"
                - "            rpm -Uvh ${RESOURCE_DIR}/${JDK_RPM} \n"
                - "            # config \n"
                - "            export JAVA_HOME=$(find / -name jdk1.8.0_*) \n"
                - "            echo \"export JAVA_HOME=${JAVA_HOME}\" >>${BASH_PATH}\
                  \ \n"
                - "            echo \"export JRE_HOME=${JAVA_HOME}/jre\" >>${BASH_PATH}\
                  \ \n"
                - "            echo \"export CLASSPATH=.:${JAVA_HOME}/lib:${JAVA_HOME}/jre/lib\"\
                  \ >>${BASH_PATH} \n"
                - "            echo \"export PATH=${JAVA_HOME}/bin:$PATH\" >>${BASH_PATH}\
                  \ \n"
                - "            source ${BASH_PATH} \n"
                - "            recordLog \"Config java successful\" \n"
                - "        } \n"
                - "         \n"
                - "        installAndConfigHadoop(){ \n"
                - "            # download \n"
                - "            tar -zxvf ${RESOURCE_DIR}/${HADOOP_GZ} -C ${ENV_DIR}\
                  \ && cd ${ENV_DIR} && mv ${HADOOP_EXTRACT_NAME} ${HADOOP} \n"
                - "            echo \"export HADOOP_HOME=${HADOOP_HOME}\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export PATH=${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH\"\
                  \ >> ${BASH_PATH} \n"
                - "            source ${BASH_PATH} \n"
                - "            # config core-site.xml \n"
                - "            mv ${HADOOP_HOME}/etc/hadoop/core-site.xml ${HADOOP_HOME}/etc/hadoop/bak.core-site.xml\
                  \ \n"
                - '            cat >${HADOOP_HOME}/etc/hadoop/core-site.xml <<EOF

                  '
                - "        <configuration> \n"
                - "          <property> \n"
                - "            <name>fs.defaultFS</name> \n"
                - "            <value>hdfs://${MASTER_IP}:9000</value> \n"
                - "          </property> \n"
                - "          <property> \n"
                - "            <name>hadoop.tmp.dir</name> \n"
                - "            <value>${ENV_DIR}/tmp</value> \n"
                - "          </property> \n"
                - "        </configuration> \n"
                - '        EOF

                  '
                - "            # hdfs-site.xml \n"
                - "            mv ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml ${HADOOP_HOME}/etc/hadoop/bak.hdfs-site.xml\
                  \ \n"
                - '            cat >${HADOOP_HOME}/etc/hadoop/hdfs-site.xml <<EOF

                  '
                - "        <configuration> \n"
                - "          <property> \n"
                - "            <name>dfs.replication</name> \n"
                - "            <value>${NODE_COUNT}</value> \n"
                - "          </property> \n"
                - "        </configuration> \n"
                - '        EOF

                  '
                - "            # config mapred-site.xml \n"
                - '            cat >${HADOOP_HOME}/etc/hadoop/mapred-site.xml <<EOF

                  '
                - "        <configuration> \n"
                - "          <property> \n"
                - "            <name>mapreduce.framework.name</name> \n"
                - "            <value>yarn</value> \n"
                - "          </property> \n"
                - "        </configuration> \n"
                - '        EOF

                  '
                - "            # config yarn-site.xml \n"
                - "            mv ${HADOOP_HOME}/etc/hadoop/yarn-site.xml ${HADOOP_HOME}/etc/hadoop/bak.yarn-site.xml\
                  \ \n"
                - '            cat >${HADOOP_HOME}/etc/hadoop/yarn-site.xml <<EOF

                  '
                - "        <configuration> \n"
                - "          <property> \n"
                - "            <name>yarn.nodemanager.aux-services</name> \n"
                - "            <value>mapreduce_shuffle</value> \n"
                - "          </property> \n"
                - "          <property> \n"
                - "            <name>yarn.resourcemanager.hostname</name> \n"
                - "            <value>${MASTER_IP}</value> \n"
                - "          </property> \n"
                - "        </configuration> \n"
                - '        EOF

                  '
                - "            # config java home \n"
                - "            sed -i 's/export JAVA_HOME=/#export JAVA_HOME=/' ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh\
                  \ \n"
                - "            sed -i \"/#export JAVA_HOME=/a export JAVA_HOME=${JAVA_HOME}\"\
                  \ ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh \n"
                - "            sed -i 's/export JAVA_HOME=/#export JAVA_HOME=/' ${HADOOP_HOME}/etc/hadoop/mapred-env.sh\
                  \ \n"
                - "            sed -i \"/#export JAVA_HOME=/a export JAVA_HOME=${JAVA_HOME}\"\
                  \ ${HADOOP_HOME}/etc/hadoop/mapred-env.sh \n"
                - "            sed -i 's/export JAVA_HOME=/#export JAVA_HOME=/' ${HADOOP_HOME}/etc/hadoop/yarn-env.sh\
                  \ \n"
                - "            sed -i \"/#export JAVA_HOME=/a export JAVA_HOME=${JAVA_HOME}\"\
                  \ ${HADOOP_HOME}/etc/hadoop/yarn-env.sh \n"
                - "            recordLog \"Config hadoop successful\" \n"
                - "        } \n"
                - "         \n"
                - "        installSparkAndConfig() { \n"
                - "           tar zxvf ${RESOURCE_DIR}/${SPARK_TGZ} -C ${ENV_DIR}\
                  \ && cd ${ENV_DIR} && mv ${SPARK_EXTRACT_NAME} ${SPARK} \n"
                - "           mv ${ENV_DIR}/${SPARK}/conf/spark-env.sh.template ${ENV_DIR}/${SPARK}/conf/spark-env.sh\
                  \ \n"
                - "           echo \"export SCALA_HOME=${ENV_DIR}/scala\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
                  \ \n"
                - "           echo \"export JAVA_HOME=${JAVA_HOME}\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
                  \ \n"
                - "           echo \"export HADOOP_HOME=${ENV_DIR}/hadoop\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
                  \ \n"
                - "           echo \"export HADOOP_CONF_DIR=${ENV_DIR}/hadoop/etc/hadoop\"\
                  \ >> ${ENV_DIR}/spark/conf/spark-env.sh \n"
                - "           echo \"export SPARK_HOME=${ENV_DIR}/spark\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
                  \ \n"
                - "           echo \"export SPARK_MASTER_IP=${HOST_IP}\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
                  \ \n"
                - "           echo \"export SPARK_EXECUTOR_MEMORY=1G\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
                  \ \n"
                - "           recordLog \"Config spark successful\" \n"
                - "        } \n"
                - "         \n"
                - "        installScalaAndConfig() { \n"
                - "            tar zxvf ${RESOURCE_DIR}/${SCALA_TGZ} -C ${ENV_DIR}\
                  \ && cd ${ENV_DIR} && mv ${SCALA_EXTRACT_NAME} ${SCALA} \n"
                - "            echo \"export SCALA_HOME=${ENV_DIR}/scala\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"PATH=$PATH:${ENV_DIR}/scala/bin\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export JAVA_HOME=${JAVA_HOME}\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export JRE_HOME=${JAVA_HOME}/jre\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\"\
                  \ >> ${BASH_PATH} \n"
                - "            echo \"export PATH=${JAVA_HOME}/bin:$PATH\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export HADOOP_HOME=${ENV_DIR}/hadoop\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export PATH=.:${ENV_DIR}/hadoop/bin:$PATH\"\
                  \ >> ${BASH_PATH} \n"
                - "            echo \"export SPARK_HOME=${ENV_DIR}/spark\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export PATH=.:${ENV_DIR}/spark/bin:$PATH\" >>\
                  \ ${BASH_PATH} \n"
                - "            source ${BASH_PATH} \n"
                - "            recordLog \"Config scala successful\" \n"
                - "        } \n"
                - "         \n"
                - "        generateAddNodeScript() { \n"
                - "            echo '#!/bin/bash' >${ADD_NODES_FILE} \n"
                - "            echo 'NODE_COUNT=$1' >>${ADD_NODES_FILE} \n"
                - "            echo \"ssh root@${MASTER_IP} \\\"echo '${HOST_IP}:${HOST_NAME}'\
                  \ >> ${NODES_INFO_FILE};bash ${CLUSTER_FILE} \\${NODE_COUNT}\\\"\
                  \" >> ${ADD_NODES_FILE} \n"
                - "        } \n"
                - "         \n"
                - "        generateRmNodeScript() { \n"
                - "            echo '#!/bin/bash' >${RM_NODES_FILE} \n"
                - "            echo 'NODE_COUNT=$1' >>${RM_NODES_FILE} \n"
                - "            echo \"ssh root@${MASTER_IP} \\\"sed -i '/${HOST_IP}:${HOST_NAME}/d'\
                  \ ${NODES_INFO_FILE};echo ${HOST_IP}:${HOST_NAME} >> ${RM_NODES_INI};bash\
                  \ ${CLUSTER_FILE} \\${NODE_COUNT}\\\"\" >>${RM_NODES_FILE} \n"
                - "        } \n"
                - "         \n"
                - "        main() { \n"
                - "            prepareEnv \n"
                - "            generateSSHScript \n"
                - "            configLocalSSH \n"
                - "            scpResources \n"
                - "            installJavaAndConfig \n"
                - "            installAndConfigHadoop \n"
                - "            installSparkAndConfig \n"
                - "            installScalaAndConfig \n"
                - "            generateAddNodeScript \n"
                - "            generateRmNodeScript \n"
                - "            bash ${ADD_NODES_FILE} ${NODE_COUNT} \n"
                - "        } \n"
                - "         \n"
                - "        main \n"
                - "        ros-notify \n"
          - "\n      instanceId: '{{ ACS::TaskLoopItem }}'\n      commandType: RunShellScript\n\
            \    Loop:\n      RateControl:\n        Mode: Concurrency\n        MaxErrors:\
            \ 0\n        Concurrency: 10\n      Items: '{{ instanceIds }}'\n     \
            \ Outputs:\n        commandOutputs:\n          AggregateType: 'Fn::ListJoin'\n\
            \          AggregateField: commandOutput\n    Outputs:\n      commandOutput:\n\
            \        Type: String\n        ValueSelector: invocationOutput\n  - Name:\
            \ CompleteLifecycleActionForContinue\n    Action: 'ACS::ExecuteAPI'\n\
            \    OnSuccess: 'ACS::END'\n    Properties:\n      Service: ESS\n    \
            \  API: CompleteLifecycleAction\n      Parameters:\n        RegionId:\
            \ '{{ regionId }}'\n        LifecycleHookId: '{{ lifecycleHookId }}'\n\
            \        LifecycleActionToken: '{{ lifecycleActionToken }}'\n  - Name:\
            \ CompleteLifecycleActionForAbandon\n    Action: 'ACS::ExecuteAPI'\n \
            \   Properties:\n      Service: ESS\n      API: CompleteLifecycleAction\n\
            \      Parameters:\n        RegionId: '{{ regionId }}'\n        LifecycleHookId:\
            \ '{{ lifecycleHookId }}'\n        LifecycleActionToken: '{{ lifecycleActionToken\
            \ }}'\n        LifecycleActionResult: ABANDON"
      TemplateName:
        Fn::Join:
        - ''
        - - StackId-
          - Ref: ALIYUN::StackId
          - -Out
    DependsOn:
    - RamRole
    Metadata:
      ALIYUN::ROS::Designer:
        id: 67a74071-cd8a-4cf0-93bf-e4f05fbf4f7c
  RosWaitConditionMasterHandle:
    Type: ALIYUN::ROS::WaitConditionHandle
    Properties: {}
    Metadata:
      ALIYUN::ROS::Designer:
        id: 28c8b447-0074-4e1c-b16e-a23b999b1221
  RosWaitConditionMaster:
    Type: ALIYUN::ROS::WaitCondition
    Properties:
      Count: 1
      Handle:
        Ref: RosWaitConditionMasterHandle
      Timeout: 1800
    Metadata:
      ALIYUN::ROS::Designer:
        id: d31fecba-ebcf-42a2-b524-af819d7cc0fd
  EssScalingGroupSlave:
    Type: ALIYUN::ESS::ScalingGroup
    Properties:
      VSwitchId:
        Ref: VSwitchId
      DefaultCooldown: 0
      DesiredCapacity:
        Fn::Calculate:
        - '{0}-1'
        - 0
        - - Ref: Amount
      HealthCheckType: ECS
      MaxSize:
        Ref: Amount
      MinSize: 2
      RemovalPolicys:
      - NewestInstance
      - OldestScalingConfiguration
      ScalingGroupName:
        Fn::Join:
        - '-'
        - - Spark-ros
          - Ref: ALIYUN::StackId
          - Ref: Amount
    DependsOn:
    - EcsInstanceGroupMaster
    - OOSTemplateIn
    - OOSTemplateOut
    - RosWaitConditionMaster
    Metadata:
      ALIYUN::ROS::Designer:
        id: c5ebe395-2e00-4f5c-9656-4db3c6a06f6d
  EssLifecycleHookIn:
    Type: ALIYUN::ESS::LifecycleHook
    Properties:
      DefaultResult: CONTINUE
      HeartbeatTimeout: 600
      LifecycleTransition: SCALE_IN
      NotificationArn:
        Fn::Join:
        - ''
        - - 'acs:ess:'
          - Ref: ALIYUN::Region
          - ':'
          - Ref: ALIYUN::TenantId
          - :oos/
          - Fn::GetAtt:
            - OOSTemplateIn
            - TemplateName
      NotificationMetadata:
        Fn::Join:
        - ''
        - - '{"regionId": "${regionId}","instanceIds": "${instanceIds}","lifecycleHookId":
            "${lifecycleHookId}","lifecycleActionToken": "${lifecycleActionToken}"}'
      ScalingGroupId:
        Ref: EssScalingGroupSlave
    DependsOn:
    - EssScalingGroupSlave
    - OOSTemplateIn
    - OOSTemplateOut
    Metadata:
      ALIYUN::ROS::Designer:
        id: 3c848940-437b-4576-96e0-4514bedb2d90
  EssLifecycleHookOut:
    Type: ALIYUN::ESS::LifecycleHook
    Properties:
      DefaultResult: CONTINUE
      HeartbeatTimeout: 600
      LifecycleTransition: SCALE_OUT
      NotificationArn:
        Fn::Join:
        - ''
        - - 'acs:ess:'
          - Ref: ALIYUN::Region
          - ':'
          - Ref: ALIYUN::TenantId
          - :oos/
          - Fn::GetAtt:
            - OOSTemplateOut
            - TemplateName
      NotificationMetadata:
        Fn::Join:
        - ''
        - - '{"regionId": "${regionId}","instanceIds": "${instanceIds}","lifecycleHookId":
            "${lifecycleHookId}","lifecycleActionToken": "${lifecycleActionToken}"}'
      ScalingGroupId:
        Ref: EssScalingGroupSlave
    DependsOn:
    - EssScalingGroupSlave
    - OOSTemplateOut
    Metadata:
      ALIYUN::ROS::Designer:
        id: 6163867a-1838-4f86-bb06-ca736e88eb68
  EssScalingConfigurationSlave:
    Type: ALIYUN::ESS::ScalingConfiguration
    Properties:
      SecurityGroupId:
        Ref: SecurityGroupId
      ImageId: centos_7_06_64_20G_alibase_20190711.vhd
      InstanceName:
        Fn::Join:
        - '-'
        - - Spark-node
          - Ref: ALIYUN::StackId
      InstanceTypes:
      - Ref: InstanceType
      IoOptimized: optimized
      ScalingConfigurationName:
        Fn::Join:
        - '-'
        - - Spark-ros
          - Ref: ALIYUN::StackId
          - Ref: Amount
      ScalingGroupId:
        Ref: EssScalingGroupSlave
      SystemDiskCategory:
        Ref: DiskCategory
      SystemDiskSize:
        Ref: DiskSize
    DependsOn:
    - EssScalingGroupSlave
    Metadata:
      ALIYUN::ROS::Designer:
        id: 440cd165-f824-4c69-8005-2ba8b4bfbe47
  EssScalingGroupEnable:
    Type: ALIYUN::ESS::ScalingGroupEnable
    Properties:
      ScalingConfigurationId:
        Ref: EssScalingConfigurationSlave
      ScalingGroupId:
        Ref: EssScalingGroupSlave
    DependsOn:
    - EcsInstanceGroupMaster
    - EssScalingConfigurationSlave
    - RosWaitConditionMaster
    Metadata:
      ALIYUN::ROS::Designer:
        id: 866c1f6a-2b7a-49c3-be21-17c039d49eb4
  RosWaitConditionClusterHandle:
    Type: ALIYUN::ROS::WaitConditionHandle
    Properties: {}
    Metadata:
      ALIYUN::ROS::Designer:
        id: 96189890-7475-4a66-956e-0a7bf79f1832
  RosWaitConditionCluster:
    Type: ALIYUN::ROS::WaitCondition
    Properties:
      Count: 1
      Handle:
        Ref: RosWaitConditionClusterHandle
      Timeout: 3600
    Metadata:
      ALIYUN::ROS::Designer:
        id: ac2da7a5-324d-445d-b539-670f51aa4211
  RosWaitConditionHandleEss:
    Type: ALIYUN::ROS::WaitConditionHandle
    Properties: {}
    Metadata:
      ALIYUN::ROS::Designer:
        id: 7a568f88-588f-4635-b081-f327f41d685a
  RosWaitConditionEss:
    Type: ALIYUN::ROS::WaitCondition
    Properties:
      Count:
        Fn::Calculate:
        - '{0}-1'
        - 0
        - - Ref: Amount
      Handle:
        Ref: RosWaitConditionHandleEss
      Timeout: 1800
    Metadata:
      ALIYUN::ROS::Designer:
        id: aca6e39d-f458-420b-8506-9ca72010ea67
Outputs:
  EcsInstanceIds:
    Value:
      Fn::GetAtt:
      - EcsInstanceGroupMaster
      - InstanceIds
  EssGroupId:
    Value:
      Fn::GetAtt:
      - EssScalingGroupSlave
      - ScalingGroupId
  MasterPrivateIp:
    Value:
      Fn::Select:
      - '0'
      - Fn::GetAtt:
        - EcsInstanceGroupMaster
        - PrivateIps
  SparkWebSiteURL:
    Value:
      Fn::Join:
      - ''
      - - http://
        - Fn::Select:
          - '0'
          - Fn::GetAtt:
            - EcsInstanceGroupMaster
            - PublicIps
        - :8080
Metadata:
  ALIYUN::ROS::Interface:
    ParameterGroups:
    - Parameters:
      - VpcId
      - VSwitchZoneId
      - VSwitchId
      - SecurityGroupId
      Label:
        default:
          en: Infrastructure Configuration
          zh-cn: 基础资源配置（必填）
    - Parameters:
      - InstanceType
      - InstancePassword
      - DiskCategory
      - DiskSize
      - Amount
      Label:
        default:
          en: Spark Configuration
          zh-cn: Spark 配置（必填）
    TemplateTags:
    - acs:solution:数据分析:Spark集群版(已有VPC)
  ALIYUN::ROS::Designer:
    28c8b447-0074-4e1c-b16e-a23b999b1221:
      position:
        x: -177
        y: 125
      size:
        height: 60
        width: 60
      z: 0
    29aa811e-37bf-4487-b5a4-a0479d28f65b:
      source:
        id: f38f57e5-4fad-404f-9550-45331dca1d60
      target:
        id: c40eb24b-0042-4ad8-b85c-b6155fa52238
      z: 1
    3c848940-437b-4576-96e0-4514bedb2d90:
      position:
        x: -5
        y: 344
      size:
        height: 60
        width: 60
      z: 0
    440cd165-f824-4c69-8005-2ba8b4bfbe47:
      position:
        x: 85
        y: 23
      size:
        height: 60
        width: 60
      z: 0
    4d17593b-029a-45bf-849a-38376b2ac955:
      position:
        x: -227
        y: 202
      size:
        height: 60
        width: 60
      z: 0
    56eeaf99-c2dc-4fbc-83f1-cb8eebc74a54:
      source:
        id: 3c848940-437b-4576-96e0-4514bedb2d90
      target:
        id: f38f57e5-4fad-404f-9550-45331dca1d60
      z: 1
    6163867a-1838-4f86-bb06-ca736e88eb68:
      position:
        x: 184
        y: 344
      size:
        height: 60
        width: 60
      z: 0
    676fcf93-a391-4091-b275-fda3dde5e00f:
      source:
        id: 3c848940-437b-4576-96e0-4514bedb2d90
      target:
        id: c5ebe395-2e00-4f5c-9656-4db3c6a06f6d
      z: 1
    67a74071-cd8a-4cf0-93bf-e4f05fbf4f7c:
      position:
        x: 184
        y: 455
      size:
        height: 60
        width: 60
      z: 0
    6d165595-5940-4434-a9c1-f7f949a28319:
      source:
        id: aca6e39d-f458-420b-8506-9ca72010ea67
      target:
        id: 7a568f88-588f-4635-b081-f327f41d685a
      z: 1
    6fd03136-d325-45b4-a36e-35817e61bf55:
      source:
        id: 866c1f6a-2b7a-49c3-be21-17c039d49eb4
      target:
        id: 440cd165-f824-4c69-8005-2ba8b4bfbe47
      z: 1
    76a7a3dd-3fbd-49d9-b414-e3727db7ce8a:
      source:
        id: d31fecba-ebcf-42a2-b524-af819d7cc0fd
      target:
        id: 28c8b447-0074-4e1c-b16e-a23b999b1221
      z: 1
    7a568f88-588f-4635-b081-f327f41d685a:
      position:
        x: 245
        y: 216
      size:
        height: 60
        width: 60
      z: 0
    866c1f6a-2b7a-49c3-be21-17c039d49eb4:
      position:
        x: -61
        y: 106
      size:
        height: 60
        width: 60
      z: 0
    8798c069-8f51-492f-9f68-63daf0f63476:
      source:
        id: 866c1f6a-2b7a-49c3-be21-17c039d49eb4
      target:
        id: c5ebe395-2e00-4f5c-9656-4db3c6a06f6d
      z: 1
    96189890-7475-4a66-956e-0a7bf79f1832:
      position:
        x: -174
        y: 289
      size:
        height: 60
        width: 60
      z: 0
    a3eded63-ff54-4d86-8fd8-ca9cace28183:
      source:
        id: 440cd165-f824-4c69-8005-2ba8b4bfbe47
      target:
        id: c5ebe395-2e00-4f5c-9656-4db3c6a06f6d
      z: 1
    a82a948b-5ba6-4adb-94e8-34fe8db27c79:
      source:
        id: 6163867a-1838-4f86-bb06-ca736e88eb68
      target:
        id: c5ebe395-2e00-4f5c-9656-4db3c6a06f6d
      z: 1
    abf654b3-9590-4a77-9944-24093cf9b09f:
      source:
        id: fabc6711-b5c0-4aef-a1fc-fae3761b74a8
      target:
        id: 4d17593b-029a-45bf-849a-38376b2ac955
      z: 1
    ac2da7a5-324d-445d-b539-670f51aa4211:
      position:
        x: -344
        y: 289
      size:
        height: 60
        width: 60
      z: 0
    aca6e39d-f458-420b-8506-9ca72010ea67:
      position:
        x: 245
        y: 120
      size:
        height: 60
        width: 60
      z: 0
    b6236b25-2ebe-476e-8ac5-4966ebf40a81:
      source:
        id: ac2da7a5-324d-445d-b539-670f51aa4211
      target:
        id: 96189890-7475-4a66-956e-0a7bf79f1832
      z: 1
    b65e9c4a-0344-406a-8b71-02e0a5eb4a2e:
      source:
        id: 67a74071-cd8a-4cf0-93bf-e4f05fbf4f7c
      target:
        id: c40eb24b-0042-4ad8-b85c-b6155fa52238
      z: 1
    bfce6520-baca-42af-9a93-883f048d3f4c:
      source:
        id: fabc6711-b5c0-4aef-a1fc-fae3761b74a8
      target:
        id: c97c38d5-253b-4d8d-89f0-1537687b31b8
      z: 1
    c40eb24b-0042-4ad8-b85c-b6155fa52238:
      position:
        x: 90
        y: 565
      size:
        height: 60
        width: 60
      z: 0
    c5ebe395-2e00-4f5c-9656-4db3c6a06f6d:
      position:
        x: 85
        y: 188
      size:
        height: 60
        width: 60
      z: 0
    c97c38d5-253b-4d8d-89f0-1537687b31b8:
      position:
        x: -526
        y: 202
      size:
        height: 60
        width: 60
      z: 0
    d31fecba-ebcf-42a2-b524-af819d7cc0fd:
      position:
        x: -346
        y: 125
      size:
        height: 60
        width: 60
      z: 0
    e05dae61-c0f3-45fd-84e1-4b709d8a6da6:
      source:
        id: 6163867a-1838-4f86-bb06-ca736e88eb68
      target:
        id: 67a74071-cd8a-4cf0-93bf-e4f05fbf4f7c
      z: 1
    f38f57e5-4fad-404f-9550-45331dca1d60:
      position:
        x: -5
        y: 455
      size:
        height: 60
        width: 60
      z: 0
    fabc6711-b5c0-4aef-a1fc-fae3761b74a8:
      position:
        x: -377
        y: 202
      size:
        height: 60
        width: 60
      z: 0
