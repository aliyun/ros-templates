ROSTemplateFormatVersion: '2015-09-01'
Description:
  zh-cn: 在现有VPC基础上，通过ROS模板部署Spark集群，含管理节点与弹性伸缩节点。配置Java 8, Hadoop 2.7.7, Scala 2.12.1,
    Spark 2.1.0环境，并设置安全组8080端口以访问管理界面。
  en: On top of the existing VPC infrastructure, deploy a Spark cluster using ROS
    (Resource Orchestration Service) templates, comprising both management nodes and
    auto-scaling nodes. Configure the environment with Java 8, Hadoop 2.7.7, Scala
    2.12.1, and Spark 2.1.0. Additionally, configure the security group to open port
    8080 for accessing the management interface.
Parameters:
  VpcId:
    Type: String
    Label:
      en: Existing VPC Instance ID
      zh-cn: 现有VPC的实例ID
    Description:
      en: Please search the ID starting with (vpc-xxx)from console-Virtual Private
        Cloud
      zh-cn: 控制台-VPC-专有网络下查询
    AssociationProperty: ALIYUN::ECS::VPC::VPCId
  VSwitchZoneId:
    Type: String
    Label:
      en: VSwitch Zone ID
      zh-cn: 交换机可用区
    Description:
      en: New Switch Availability Zone ID
      zh-cn: 新建交换机Switch的可用区ID
    AssociationProperty: ALIYUN::ECS::Instance::ZoneId
  VSwitchId:
    Type: String
    Label:
      en: VSwitch ID
      zh-cn: 网络交换机ID
    Description:
      en: Please search the business VSwitch ID starting with(vsw-xxx)from console-Virtual
        Private Cloud-VSwitches
      zh-cn: 现有业务网络交换机的实例ID,控制台-VPC-专有网络-交换机下查询
    AssociationProperty: ALIYUN::ECS::VSwitch::VSwitchId
    AssociationPropertyMetadata:
      VpcId: VpcId
  SecurityGroupId:
    Type: String
    Label:
      en: Business Security Group ID
      zh-cn: 业务安全组ID
    Description:
      en: Please search the business security group ID starting with(sg-xxx)from console-ECS-Network
        & Security
      zh-cn: 现有业务安全组的实例ID,控制台-ECS-网络与安全-安全组下查询
    AssociationProperty: ALIYUN::ECS::SecurityGroup::SecurityGroupId
    AssociationPropertyMetadata:
      VpcId: VpcId
  InstanceType:
    Type: String
    Label:
      en: Instance Type
      zh-cn: 实例规格
    Description:
      en: <font color='blue'><b>1.Before selecting the model please confirm that the
        current available zone under the model is in stock, some models need to be
        reported in advance</b></font>]<br><font color='blue'><b>2.List of optional
        models</font>]<br></b></font>[ecs.c5.large <font color='green'>2vCPU 4GiB
        Intranet bandwidth1Gbps In-grid sending and receiving packages30MillionPPSS</font>]<br></b>[ecs.c5.xlarge
        <font color='green'>4vCPU 8GiB Intranet bandwidth1.5Gbps In-grid sending and
        receiving packages50MillionPPS</font>]<br></b>[ecs.c5.2xlarge <font color='green'>8vCPU
        16GiB Intranet bandwidth2.5Gbps In-grid sending and receiving packages80MillionPPS</font>]
      zh-cn: <font color='blue'><b>1.选择机型前请先确认当前可用区下该机型是否有货，部分机型需要提前报备</b></font><br><font
        color='blue'><b>2.可选机型列表</font><br></b></font>[ecs.c5.large <font color='green'>2vCPU
        4GiB 内网带宽1Gbps 内网收发包30万PPS</font>]<br></b>[ecs.c5.xlarge <font color='green'>4vCPU
        8GiB 内网带宽1.5Gbps 内网收发包50万PPS</font>]<br></b>[ecs.c5.2xlarge <font color='green'>8vCPU
        16GiB 内网带宽2.5Gbps 内网收发包80万PPS</font>]
    AssociationProperty: ALIYUN::ECS::Instance::InstanceType
    AssociationPropertyMetadata:
      ZoneId: VSwitchZoneId
  InstancePassword:
    Type: String
    Label:
      en: Instance Password
      zh-cn: 实例密码
    Description:
      en: Server login password, Length 8-30, must contain three(Capital letters,
        lowercase letters, numbers, ()`~!@#$%^&*_-+=|{}[]:;'<>,.?/ Special symbol
        in)
      zh-cn: 服务器登录密码,长度8-30，必须包含三项（大写字母、小写字母、数字、 ()`~!@#$%^&*_-+=|{}[]:;'<>,.?/ 中的特殊符号）
    ConstraintDescription:
      en: Length 8-30, must contain three(Capital letters, lowercase letters, numbers,
        ()`~!@#$%^&*_-+=|{}[]:;'<>,.?/ Special symbol in).
      zh-cn: 长度8-30，必须包含三项（大写字母、小写字母、数字、 ()`~!@#$%^&*_-+=|{}[]:;'<>,.?/ 中的特殊符号）。
    AssociationProperty: ALIYUN::ECS::Instance::Password
  DiskCategory:
    Type: String
    Label:
      en: Disk Type
      zh-cn: 磁盘类型
    Description:
      en: '<font color=''blue''><b>Optional values:</b></font><br>[cloud_efficiency:
        <font color=''green''>Efficient Cloud Disk</font>]<br>[cloud_ssd: <font color=''green''>SSD
        Cloud Disk</font>]'
      zh-cn: '<font color=''blue''><b>可选值：</b></font><br>[cloud_efficiency: <font
        color=''green''>高效云盘</font>]<br>[cloud_ssd: <font color=''green''>SSD云盘</font>]'
    Default: cloud_efficiency
    AllowedValues:
    - cloud_efficiency
    - cloud_ssd
  DiskSize:
    Type: Number
    Label:
      en: System Disk Space
      zh-cn: 系统盘空间
    Description:
      en: ''
      zh-cn: 实例系统盘大小，单位为GiB。取值范围：20~500
    Default: 40
    MinValue: 20
    MaxValue: 500
  Amount:
    Type: Number
    Label:
      en: Instance Amount
      zh-cn: 实例数量
    Description:
      en: 'ECS Instance Amount, Allowed value: 3~10'
      zh-cn: 购买实例数量，允许值：3~10
    Default: 3
    MinValue: 3
    MaxValue: 10
  InstanceImageId:
    Type: String
    Label:
      en: Image ID
      zh-cn: 镜像ID
    Description:
      en: Image ID，See detail：<b><a href='https://www.alibabacloud.com/help/doc-detail/112977.html'
        target='_blank'><font color='blue'>Find the mirror</font></a></b>
      zh-cn: 镜像ID, 详见：<b><a href='https://help.aliyun.com/document_detail/112977.html'
        target='_blank'><font color='blue'>查找镜像</font></a></b>
    Default: centos_7
Resources:
  RamRole:
    Type: ALIYUN::RAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Action: sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - oos.aliyuncs.com
        Version: '1'
      Policies:
      - PolicyDocument:
          Statement:
          - Action:
            - ecs:*
            Effect: Allow
            Resource:
            - '*'
          - Action:
            - vpc:DescribeVpcs
            - vpc:DescribeVSwitches
            Effect: Allow
            Resource:
            - '*'
          - Action:
            - ess:CompleteLifecycleAction
            Effect: Allow
            Resource:
            - '*'
          Version: '1'
        PolicyName:
          Fn::Join:
          - ''
          - - StackId-
            - Ref: ALIYUN::StackId
      RoleName:
        Fn::Join:
        - ''
        - - StackId-
          - Ref: ALIYUN::StackId
  OOSTemplateIn:
    Type: ALIYUN::OOS::Template
    Properties:
      Content:
        Fn::Join:
        - ''
        - - '{"FormatVersion": "OOS-2019-06-01","Parameters": {"regionId": {"Type":
            "String","Default": "'
          - Ref: ALIYUN::Region
          - '"},"instanceIds": {"Type": "List","Default": ["${instanceId}"]},"lifecycleHookId":
            {"Type": "String","Default": "${lifecycleHookId}"},"lifecycleActionToken":
            {"Type": "String","Default": "${lifecycleActionToken}"}},"RamRole": "'
          - Fn::GetAtt:
            - RamRole
            - RoleName
          - '","Tasks": [{"Name": "runCommand","Action": "ACS::ECS::RunCommand","OnError":
            "CompleteLifecycleActionForAbandon","OnSuccess": "CompleteLifecycleActionForContinue","Properties":
            {"regionId": "{{ regionId }}","commandContent": "'
          - 'cd /software && rm -f /etc/hosts && rm -f /software/spark/conf/slaves
            && bash /software/spark/sbin/stop-slave.sh && bash rm_nodes.sh '
          - Ref: Amount
          - ' && sleep 15'
          - '","instanceId": "{{ ACS::TaskLoopItem }}","commandType": "RunShellScript"},
            "Loop": {"RateControl": {"Mode":"Concurrency","MaxErrors":0,"Concurrency":10},"Items":
            "{{ instanceIds }}","Outputs": {"commandOutputs": {"AggregateType": "Fn::ListJoin","AggregateField":
            "commandOutput"}}},"Outputs": {"commandOutput": {"Type": "String","ValueSelector":
            "invocationOutput"}}}, {"Name": "CompleteLifecycleActionForContinue","Action":
            "ACS::ExecuteAPI","OnSuccess": "ACS::END","Properties": {"Service": "ESS","API":
            "CompleteLifecycleAction","Parameters": {"RegionId": "{{ regionId }}","LifecycleHookId":
            "{{ lifecycleHookId }}","LifecycleActionToken": "{{ lifecycleActionToken
            }}"}}}, {"Name": "CompleteLifecycleActionForAbandon","Action": "ACS::ExecuteAPI","Properties":
            {"Service": "ESS","API": "CompleteLifecycleAction","Parameters": {"RegionId":
            "{{ regionId }}","LifecycleHookId": "{{ lifecycleHookId }}","LifecycleActionToken":
            "{{ lifecycleActionToken }}","LifecycleActionResult": "ABANDON"}}}]}'
      TemplateName:
        Fn::Join:
        - ''
        - - StackId-
          - Ref: ALIYUN::StackId
          - -In
    DependsOn:
    - RamRole
  EcsInstanceGroupMaster:
    Type: ALIYUN::ECS::InstanceGroup
    Properties:
      ZoneId:
        Ref: VSwitchZoneId
      VpcId:
        Ref: VpcId
      VSwitchId:
        Ref: VSwitchId
      SecurityGroupId:
        Ref: SecurityGroupId
      ImageId:
        Ref: InstanceImageId
      AllocatePublicIP: true
      HostName:
        Fn::Join:
        - ''
        - - Spark
          - -[0,3]
      InstanceChargeType: PostPaid
      InstanceName:
        Fn::Join:
        - ''
        - - Spark
          - -[0,3]
      InstanceType:
        Ref: InstanceType
      IoOptimized: optimized
      MaxAmount: 1
      Password:
        Ref: InstancePassword
      SystemDiskCategory:
        Ref: DiskCategory
      UserData:
        Fn::Replace:
        - ros-notify:
            Fn::GetAtt:
            - RosWaitConditionMasterHandle
            - CurlCli
        - Fn::Join:
          - ''
          - - "#!/bin/sh \n"
            - PASSWORD="
            - Ref: InstancePassword
            - "\" \n"
            - NODE_COUNT=
            - Ref: Amount
            - " \n"
            - ROS_NOTIFY="
            - Fn::GetAtt:
              - RosWaitConditionClusterHandle
              - CurlCli
            - "\" \n"
            - "sleep 10 \n"
            - "set -e \n"
            - "HOST_IP=$(ifconfig eth0 | awk '/inet /{print $2}') \n"
            - "HOST_NAME=$(hostname) \n"
            - "OSS_NAME=\"ros-template-resources\" \n"
            - "OSS_REGION=\"cn-beijing\" \n"
            - "ENDPOINT=\"aliyuncs.com\" \n"
            - "ENV_DIR=\"/software\" \n"
            - "RESOURCE_DIR=\"${ENV_DIR}/resources\" \n"
            - "BASH_PATH=\"/etc/profile\" \n"
            - "JDK_NAME=\"jdk-8u251-linux-i586\" \n"
            - "JDK_RPM=\"${JDK_NAME}.rpm\" \n"
            - "HADOOP=\"hadoop\" \n"
            - "HADOOP_EXTRACT_NAME=\"${HADOOP}-2.7.7\" \n"
            - "HADOOP_GZ=\"${HADOOP_EXTRACT_NAME}.tar.gz\" \n"
            - "SPARK=\"spark\" \n"
            - "SPARK_EXTRACT_NAME=\"${SPARK}-2.1.0-bin-hadoop2.7\" \n"
            - "SPARK_TGZ=\"${SPARK_EXTRACT_NAME}.tgz\" \n"
            - "SCALA=\"scala\" \n"
            - "SCALA_EXTRACT_NAME=\"${SCALA}-2.12.1\" \n"
            - "SCALA_TGZ=\"${SCALA_EXTRACT_NAME}.tgz\" \n"
            - "objectList=(\"JDK/${JDK_RPM}\" \"Hadoop/${SPARK_TGZ}\" \"Hadoop/${HADOOP_GZ}\"\
              \ \"Hadoop/${SCALA_TGZ}\") \n"
            - " \n"
            - "LOG_FILE=\"${ENV_DIR}/userdata.log\" \n"
            - "CLUSTER_FILE=\"${ENV_DIR}/cluster.sh\" \n"
            - "SSH_SCRIPT_FILE=\"${ENV_DIR}/ssh.sh\" \n"
            - "RM_NODES_FILE=\"${ENV_DIR}/rm_nodes.ini\" \n"
            - "ADD_NODES_FILE=\"${ENV_DIR}/add_nodes.ini\" \n"
            - "NODES_INFO_FILE=\"${ENV_DIR}/nodes_info.ini\" \n"
            - " \n"
            - "HADOOP_HOME=${ENV_DIR}/hadoop \n"
            - " \n"
            - "recordLog() { \n"
            - "    time=$(date \"+%Y-%m-%d %H:%M:%S\") \n"
            - "    echo \"$time --- $1\" >>${LOG_FILE} \n"
            - "} \n"
            - " \n"
            - "createDir() { \n"
            - "    dir=$1 \n"
            - "    if [ -d \"${dir}\" ]; then \n"
            - "        recordLog \"Create failed, dir-${dir} is existed\" \n"
            - "    else \n"
            - "        mkdir -p \"${dir}\" \n"
            - "        recordLog \"Create Dir-${dir} successful\" \n"
            - "    fi \n"
            - "} \n"
            - " \n"
            - "prepareEnv() { \n"
            - "    mkdir ${ENV_DIR} \n"
            - "    createDir ${RESOURCE_DIR} \n"
            - "} \n"
            - " \n"
            - "download() { \n"
            - "    objectName=$1 \n"
            - "    wget https://${OSS_NAME}.oss-${OSS_REGION}.${ENDPOINT}/${objectName}\
              \ -P ${RESOURCE_DIR} \n"
            - "    recordLog \"Download ${objectName} successful\" \n"
            - "} \n"
            - " \n"
            - "downloadResources() { \n"
            - "    for object in ${objectList[@]}; do \n"
            - "        download ${object} \n"
            - "    done \n"
            - "    recordLog \"Download all resources successful\" \n"
            - "} \n"
            - " \n"
            - "installJavaAndConfig() { \n"
            - "    yum -y install glibc.i686 \n"
            - "    rpm -Uvh ${RESOURCE_DIR}/${JDK_RPM} \n"
            - "    # config \n"
            - "    export JAVA_HOME=$(find / -name jdk1.8.0_*) \n"
            - "    echo \"export JAVA_HOME=${JAVA_HOME}\" >>${BASH_PATH} \n"
            - "    echo \"export JRE_HOME=${JAVA_HOME}/jre\" >>${BASH_PATH} \n"
            - "    echo \"export CLASSPATH=.:${JAVA_HOME}/lib:${JAVA_HOME}/jre/lib\"\
              \ >>${BASH_PATH} \n"
            - "    echo \"export PATH=${JAVA_HOME}/bin:$PATH\" >>${BASH_PATH} \n"
            - "    source ${BASH_PATH} \n"
            - "    recordLog \"Config java successful\" \n"
            - "} \n"
            - " \n"
            - "generateScript() { \n"
            - "    ssh-keygen -t rsa -P '' -f '/root/.ssh/id_rsa' \n"
            - "    yum -y install expect \n"
            - "    echo '#!/bin/bash' >${SSH_SCRIPT_FILE} \n"
            - "    echo 'name_or_ip=$1' >>${SSH_SCRIPT_FILE} \n"
            - "    echo 'authorized_key=$2' >>${SSH_SCRIPT_FILE} \n"
            - "    echo 'expect <<EOF' >>${SSH_SCRIPT_FILE} \n"
            - "    echo 'set timeout 150' >>${SSH_SCRIPT_FILE} \n"
            - "    echo \"spawn ssh root@\\${name_or_ip} echo \\\"\\${authorized_key}\\\
              \" >> /root/.ssh/authorized_keys\" >>${SSH_SCRIPT_FILE} \n"
            - "    echo 'expect {' >>${SSH_SCRIPT_FILE} \n"
            - "    echo \"  \\\"*yes/no*\\\" { send \\\"yes\\n\\\"; exp_continue }\"\
              \ >>${SSH_SCRIPT_FILE} \n"
            - "    echo \"  \\\"*password:\\\" { send \\\"${PASSWORD}\\n\\\" }\" >>${SSH_SCRIPT_FILE}\
              \ \n"
            - "    echo '}' >>${SSH_SCRIPT_FILE} \n"
            - "    echo 'expect eof' >>${SSH_SCRIPT_FILE} \n"
            - "    echo 'EOF' >>${SSH_SCRIPT_FILE} \n"
            - "    chmod +x ${SSH_SCRIPT_FILE} \n"
            - "    recordLog \"Generate ${SSH_SCRIPT_FILE} successful\" \n"
            - "    echo '#!/bin/bash' >\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo 'host_ip=$1' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo 'expect <<EOF' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo 'set timeout 150' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo \"spawn ssh root@\\${host_ip} exit;\" >>\"${SSH_SCRIPT_FILE}.login\"\
              \ \n"
            - "    echo 'expect {' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo \"  \\\"*yes/no*\\\" { send \\\"yes\\n\\\" }\" >>\"${SSH_SCRIPT_FILE}.login\"\
              \ \n"
            - "    echo '}' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo 'expect eof' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    echo 'EOF' >>\"${SSH_SCRIPT_FILE}.login\" \n"
            - "    chmod +x \"${SSH_SCRIPT_FILE}.login\" \n"
            - "    recordLog \"Generate ${SSH_SCRIPT_FILE}.login successful\" \n"
            - "} \n"
            - " \n"
            - "configLocalSSH() { \n"
            - "    authorized_key=$(cat /root/.ssh/id_rsa.pub) \n"
            - "    bash ${SSH_SCRIPT_FILE} \"${HOST_IP}\" \"${authorized_key}\" \n"
            - "    bash \"${SSH_SCRIPT_FILE}.login\" \"0.0.0.0\" \n"
            - "    bash \"${SSH_SCRIPT_FILE}.login\" \"localhost\" \n"
            - "    recordLog \"Config expect-localhost successful\" \n"
            - "} \n"
            - " \n"
            - "installAndConfigHadoop(){ \n"
            - "    # download \n"
            - "    tar -zxvf ${RESOURCE_DIR}/${HADOOP_GZ} -C ${ENV_DIR} && cd ${ENV_DIR}\
              \ && mv ${HADOOP_EXTRACT_NAME} ${HADOOP} \n"
            - "    echo \"export HADOOP_HOME=${HADOOP_HOME}\" >> ${BASH_PATH} \n"
            - "    echo \"export PATH=${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH\"\
              \ >> ${BASH_PATH} \n"
            - "    source ${BASH_PATH} \n"
            - "    # config core-site.xml \n"
            - "    mv ${HADOOP_HOME}/etc/hadoop/core-site.xml ${HADOOP_HOME}/etc/hadoop/bak.core-site.xml\
              \ \n"
            - '    cat >${HADOOP_HOME}/etc/hadoop/core-site.xml <<EOF

              '
            - "<configuration> \n"
            - "  <property> \n"
            - "    <name>fs.defaultFS</name> \n"
            - "    <value>hdfs://${HOST_IP}:9000</value> \n"
            - "  </property> \n"
            - "  <property> \n"
            - "    <name>hadoop.tmp.dir</name> \n"
            - "    <value>${ENV_DIR}/tmp</value> \n"
            - "  </property> \n"
            - "</configuration> \n"
            - 'EOF

              '
            - "    # hdfs-site.xml \n"
            - "    mv ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml ${HADOOP_HOME}/etc/hadoop/bak.hdfs-site.xml\
              \ \n"
            - '    cat >${HADOOP_HOME}/etc/hadoop/hdfs-site.xml <<EOF

              '
            - "<configuration> \n"
            - "  <property> \n"
            - "    <name>dfs.replication</name> \n"
            - "    <value>${NODE_COUNT}</value> \n"
            - "  </property> \n"
            - "</configuration> \n"
            - 'EOF

              '
            - "    # config mapred-site.xml \n"
            - '    cat >${HADOOP_HOME}/etc/hadoop/mapred-site.xml <<EOF

              '
            - "<configuration> \n"
            - "  <property> \n"
            - "    <name>mapreduce.framework.name</name> \n"
            - "    <value>yarn</value> \n"
            - "  </property> \n"
            - "</configuration> \n"
            - 'EOF

              '
            - "    # config yarn-site.xml \n"
            - "    mv ${HADOOP_HOME}/etc/hadoop/yarn-site.xml ${HADOOP_HOME}/etc/hadoop/bak.yarn-site.xml\
              \ \n"
            - '    cat >${HADOOP_HOME}/etc/hadoop/yarn-site.xml <<EOF

              '
            - "<configuration> \n"
            - "  <property> \n"
            - "    <name>yarn.nodemanager.aux-services</name> \n"
            - "    <value>mapreduce_shuffle</value> \n"
            - "  </property> \n"
            - "  <property> \n"
            - "    <name>yarn.resourcemanager.hostname</name> \n"
            - "    <value>${HOST_IP}</value> \n"
            - "  </property> \n"
            - "</configuration> \n"
            - 'EOF

              '
            - "    # config java home \n"
            - "    sed -i 's/export JAVA_HOME=/#export JAVA_HOME=/' ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh\
              \ \n"
            - "    sed -i \"/#export JAVA_HOME=/a export JAVA_HOME=${JAVA_HOME}\"\
              \ ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh \n"
            - "    sed -i 's/export JAVA_HOME=/#export JAVA_HOME=/' ${HADOOP_HOME}/etc/hadoop/mapred-env.sh\
              \ \n"
            - "    sed -i \"/#export JAVA_HOME=/a export JAVA_HOME=${JAVA_HOME}\"\
              \ ${HADOOP_HOME}/etc/hadoop/mapred-env.sh \n"
            - "    sed -i 's/export JAVA_HOME=/#export JAVA_HOME=/' ${HADOOP_HOME}/etc/hadoop/yarn-env.sh\
              \ \n"
            - "    sed -i \"/#export JAVA_HOME=/a export JAVA_HOME=${JAVA_HOME}\"\
              \ ${HADOOP_HOME}/etc/hadoop/yarn-env.sh \n"
            - "    recordLog \"Config hadoop successful\" \n"
            - "} \n"
            - " \n"
            - "installSparkAndConfig() { \n"
            - "   tar zxvf ${RESOURCE_DIR}/spark-*.tgz -C ${ENV_DIR} && cd ${ENV_DIR}\
              \ && mv ${SPARK_EXTRACT_NAME} ${SPARK} \n"
            - "   mv ${ENV_DIR}/${SPARK}/conf/spark-env.sh.template ${ENV_DIR}/${SPARK}/conf/spark-env.sh\
              \ \n"
            - "   echo \"export SCALA_HOME=${ENV_DIR}/scala\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
              \ \n"
            - "   echo \"export JAVA_HOME=${JAVA_HOME}\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
              \ \n"
            - "   echo \"export HADOOP_HOME=${ENV_DIR}/hadoop\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
              \ \n"
            - "   echo \"export HADOOP_CONF_DIR=${ENV_DIR}/hadoop/etc/hadoop\" >>\
              \ ${ENV_DIR}/spark/conf/spark-env.sh \n"
            - "   echo \"export SPARK_HOME=${ENV_DIR}/spark\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
              \ \n"
            - "   echo \"export SPARK_MASTER_IP=${HOST_IP}\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
              \ \n"
            - "   echo \"export SPARK_EXECUTOR_MEMORY=1G\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
              \ \n"
            - "   recordLog \"Config spark successful\" \n"
            - "} \n"
            - " \n"
            - "installScalaAndConfig() { \n"
            - "    tar zxvf ${RESOURCE_DIR}/${SCALA_TGZ} -C ${ENV_DIR} && cd ${ENV_DIR}\
              \ && mv ${SCALA_EXTRACT_NAME} ${SCALA} \n"
            - "    echo \"export SCALA_HOME=${ENV_DIR}/scala\" >> ${BASH_PATH} \n"
            - "    echo \"PATH=$PATH:${ENV_DIR}/scala/bin\" >> ${BASH_PATH} \n"
            - "    echo \"export JAVA_HOME=${JAVA_HOME}\" >> ${BASH_PATH} \n"
            - "    echo \"export JRE_HOME=${JAVA_HOME}/jre\" >> ${BASH_PATH} \n"
            - "    echo \"export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\" >>\
              \ ${BASH_PATH} \n"
            - "    echo \"export PATH=${JAVA_HOME}/bin:$PATH\" >> ${BASH_PATH} \n"
            - "    echo \"export HADOOP_HOME=${ENV_DIR}/hadoop\" >> ${BASH_PATH} \n"
            - "    echo \"export PATH=.:${ENV_DIR}/hadoop/bin:$PATH\" >> ${BASH_PATH}\
              \ \n"
            - "    echo \"export SPARK_HOME=${ENV_DIR}/spark\" >> ${BASH_PATH} \n"
            - "    echo \"export PATH=.:${ENV_DIR}/spark/bin:$PATH\" >> ${BASH_PATH}\
              \ \n"
            - "    source ${BASH_PATH} \n"
            - "    recordLog \"Config scala successful\" \n"
            - "} \n"
            - " \n"
            - " \n"
            - "generateClusterScript() { \n"
            - '    cat >${CLUSTER_FILE} <<EOF

              '
            - "#!/bin/bash \n"
            - "NODE_COUNT=\\$1 \n"
            - "HOST_IP=\\$(ifconfig eth0 | awk '/inet /{print \\$2}') \n"
            - "#sleep 10 \n"
            - "#set -e \n"
            - "ENV_DIR=\"/software\" \n"
            - "HADOOP=\"hadoop\" \n"
            - "SPARK=\"spark\" \n"
            - " \n"
            - "LOG_FILE=\"\\${ENV_DIR}/userdata.log\" \n"
            - "SSH_SCRIPT_FILE=\"\\${ENV_DIR}/ssh.sh\" \n"
            - "RM_NODES_FILE=\"\\${ENV_DIR}/rm_nodes.ini\" \n"
            - "ADD_NODES_FILE=\"\\${ENV_DIR}/add_nodes.ini\" \n"
            - "NODES_INFO_FILE=\"\\${ENV_DIR}/nodes_info.ini\" \n"
            - "NODES_COUNT_FILE=\"\\${ENV_DIR}/nodes_count.ini\" \n"
            - "HOSTS_PATH=\"/etc/hosts\" \n"
            - " \n"
            - "HADOOP_HOME=\\${ENV_DIR}/\\${HADOOP} \n"
            - "SPARK_HOME=\\${ENV_DIR}/\\${SPARK} \n"
            - " \n"
            - "configCluster() { \n"
            - "    if [ ! -f \\${NODES_COUNT_FILE} ]; then \n"
            - "        echo \"正常创建config\" >> \\${LOG_FILE} \n"
            - "        nodes_info=\\$(cat \\${NODES_INFO_FILE}) \n"
            - "        for node_info in \\${nodes_info[@]}; do \n"
            - "            new_node_info=(\\${node_info//:/ }) \n"
            - "            node_ip=\\${new_node_info[0]} \n"
            - "            node_hostname=\\${new_node_info[1]} \n"
            - "            echo \"\\${node_ip} \\${node_hostname}\" >>\\${HOSTS_PATH}\
              \ \n"
            - "            echo \"\\${node_hostname}\" >>\"\\${SPARK_HOME}/conf/slaves\"\
              \ \n"
            - "            if [[ \"\\${HOST_IP}\" == \"\\${node_ip}\" ]]; then \n"
            - "                continue \n"
            - "            fi \n"
            - "            authorized_key=\\$(cat /root/.ssh/id_rsa.pub) \n"
            - "            bash \\${SSH_SCRIPT_FILE} \"\\${node_hostname}\" \"\\${authorized_key}\"\
              \ \n"
            - "        done \n"
            - "        for node_info in \\${nodes_info[@]}; do \n"
            - "            new_node_info=(\\${node_info//:/ }) \n"
            - "            node_ip=\\${new_node_info[0]} \n"
            - "            node_hostname=\\${new_node_info[1]} \n"
            - "            if [[ \"\\${HOST_IP}\" == \"\\${node_ip}\" ]]; then \n"
            - "                continue \n"
            - "            fi \n"
            - "            scp -r \\${HOSTS_PATH} \\${node_hostname}:/etc \n"
            - "            scp -r \\${SPARK_HOME}/conf/slaves \\${node_hostname}:\\\
              ${SPARK_HOME}/conf \n"
            - "        done \n"
            - "    else \n"
            - "        old_node_count=\\$(cat \\${NODES_COUNT_FILE}) \n"
            - "        if [ \\${NODE_COUNT} -gt \\${old_node_count} ]; then \n"
            - "            nodes_info=\\$(cat \\${NODES_INFO_FILE}) \n"
            - "            for node_info in \\${nodes_info[@]}; do \n"
            - "                new_node_info=(\\${node_info//:/ }) \n"
            - "                node_ip=\\${new_node_info[0]} \n"
            - "                node_hostname=\\${new_node_info[1]} \n"
            - "                grep \"\\${node_hostname}\" \\${HOSTS_PATH} \n"
            - "                if [ \\$? -eq \"0\" ]; then \n"
            - "                    continue \n"
            - "                fi \n"
            - "                echo \"\\${node_ip} \\${node_hostname}\" >>\\${HOSTS_PATH}\
              \ \n"
            - "                echo \"\\${node_hostname}\" >>\"\\${SPARK_HOME}/conf/slaves\"\
              \ \n"
            - "                authorized_key=\\$(cat /root/.ssh/id_rsa.pub) \n"
            - "                bash \\${SSH_SCRIPT_FILE} \"\\${node_hostname}\" \"\
              \\${authorized_key}\" \n"
            - "                sed -i \"s%<value>\\${old_node_count}</value>%<value>\\\
              ${NODE_COUNT}</value>%g\" \\${HADOOP_HOME}/etc/hadoop/hdfs-site.xml\
              \ \n"
            - "            done \n"
            - "            for node_info in \\${nodes_info[@]}; do \n"
            - "                new_node_info=(\\${node_info//:/ }) \n"
            - "                node_ip=\\${new_node_info[0]} \n"
            - "                node_hostname=\\${new_node_info[1]} \n"
            - "                if [ \"\\${HOST_IP}\" == \"\\${node_ip}\" ]; then \n"
            - "                    continue \n"
            - "                fi \n"
            - "                scp -r \\${HOSTS_PATH} \\${node_hostname}:/etc \n"
            - "                scp -r \\${SPARK_HOME}/conf/slaves \\${node_hostname}:\\\
              ${SPARK_HOME}/conf \n"
            - "                scp -r \\${HADOOP_HOME}/etc/hadoop/hdfs-site.xml \\\
              ${node_hostname}:\\${HADOOP_HOME}/etc/hadoop \n"
            - "            done \n"
            - "        else \n"
            - "            rm_nodes_info=\\$(cat \\${RM_NODES_FILE}) \n"
            - "            for rm_node in \\${rm_nodes_info[@]}; do \n"
            - "                rm_node_info=(\\${rm_node//:/ }) \n"
            - "                node_ip=\\${rm_node_info[0]} \n"
            - "                node_hostname=\\${rm_node_info[1]} \n"
            - "                grep \"\\${node_hostname}\" \"\\${HOSTS_PATH}\" \n"
            - "                if [ \\$? -eq \"0\" ]; then \n"
            - "                    sed -i \"/\\${node_hostname}/d\" \\${HOSTS_PATH}\
              \ \n"
            - "                    sed -i \"/\\${node_hostname}/d\" \\${SPARK_HOME}/conf/slaves\
              \ \n"
            - "                    sed -i \"s%<value>\\${old_node_count}</value>%<value>\\\
              ${NODE_COUNT}</value>%g\" \\${HADOOP_HOME}/etc/hadoop/hdfs-site.xml\
              \ \n"
            - "                fi \n"
            - "            done \n"
            - "            nodes_info=\\$(cat \\${NODES_INFO_FILE}) \n"
            - "            for node_info in \\${nodes_info[@]}; do \n"
            - "                new_node_info=(\\${node_info//:/ }) \n"
            - "                node_ip=\\${new_node_info[0]} \n"
            - "                node_hostname=\\${new_node_info[1]} \n"
            - "                if [ \"\\${HOST_IP}\" == \"\\${node_ip}\" ]; then \n"
            - "                    continue \n"
            - "                fi \n"
            - "                scp -r \\${HOSTS_PATH} \\${node_hostname}:/etc \n"
            - "                scp -r \\${SPARK_HOME}/conf/slaves \\${node_hostname}:\\\
              ${SPARK_HOME}/conf \n"
            - "                scp -r \\${HADOOP_HOME}/etc/hadoop/hdfs-site.xml \\\
              ${node_hostname}:\\${HADOOP_HOME}/etc/hadoop \n"
            - "            done \n"
            - "        fi \n"
            - "    fi \n"
            - "} \n"
            - " \n"
            - "startCluster() { \n"
            - "    if [ ! -f \\${NODES_COUNT_FILE} ]; then \n"
            - "        echo \"正常创建start\" >> \\${LOG_FILE} \n"
            - "        \\${HADOOP_HOME}/bin/hdfs namenode -format \n"
            - "        \\${HADOOP_HOME}/sbin/start-dfs.sh \n"
            - "        \\${HADOOP_HOME}/sbin/start-yarn.sh \n"
            - "        nodes_info=\\$(cat \\${NODES_INFO_FILE}) \n"
            - "        for node_info in \\${nodes_info[@]}; do \n"
            - "            new_node_info=(\\${node_info//:/ }) \n"
            - "            node_ip=\\${new_node_info[0]} \n"
            - "            node_hostname=\\${new_node_info[1]} \n"
            - "            if [[ \"\\${HOST_IP}\" == \"\\${node_ip}\" ]]; then \n"
            - "                continue \n"
            - "            fi \n"
            - "            ssh root@\\${node_hostname} \"bash \\${ENV_DIR}/hadoop/sbin/hadoop-daemon.sh\
              \ start datanode && bash \\${ENV_DIR}/hadoop/sbin/yarn-daemon.sh start\
              \ nodemanager && exit;\" \n"
            - "        done \n"
            - "        echo \"Start hadoop successful\" >>\\${LOG_FILE} \n"
            - "        bash \\${ENV_DIR}/spark/sbin/start-all.sh \n"
            - "        echo \"Start spark successful\" >>\\${LOG_FILE} \n"
            - "    else \n"
            - "        old_node_count=\\$(cat \\${NODES_COUNT_FILE}) \n"
            - "        if [ \\${NODE_COUNT} -gt \\${old_node_count} ]; then \n"
            - "            nodes_info=\\$(cat \\${NODES_INFO_FILE}) \n"
            - "            for node_info in \\${nodes_info[@]}; do \n"
            - "                new_node_info=(\\${node_info//:/ }) \n"
            - "                node_ip=\\${new_node_info[0]} \n"
            - "                node_hostname=\\${new_node_info[1]} \n"
            - "                grep \"\\${node_hostname}\" \\${HOSTS_PATH} \n"
            - "                if [ \\$? -eq \"0\" ]; then \n"
            - "                    continue \n"
            - "                fi \n"
            - "                ssh root@\\${node_hostname} \"bash \\${ENV_DIR}/hadoop/sbin/hadoop-daemon.sh\
              \ start datanode && bash \\${ENV_DIR}/hadoop/sbin/yarn-daemon.sh start\
              \ nodemanager && exit;\" \n"
            - "            done \n"
            - "            echo \"Start hadoop(scale-out) successful\" >>\\${LOG_FILE}\
              \ \n"
            - "            bash \\${ENV_DIR}/spark/sbin/stop-all.sh \n"
            - "            sleep 3 \n"
            - "            bash \\${ENV_DIR}/spark/sbin/start-all.sh \n"
            - "            echo \"Start spark(scale-out) successful\" >>\\${LOG_FILE}\
              \ \n"
            - "            rm -f \\${ADD_NODES_FILE} \n"
            - "        else \n"
            - "            nodes_info=\\$(cat \\${NODES_INFO_FILE}) \n"
            - "            echo \"Start hadoop(scale-in) successful\" >>\\${LOG_FILE}\
              \ \n"
            - "            bash \\${ENV_DIR}/spark/sbin/stop-all.sh \n"
            - "            sleep 10 \n"
            - "            bash \\${ENV_DIR}/spark/sbin/start-all.sh \n"
            - "            echo \"Start spark(scale-in) successful\" >>\\${LOG_FILE}\
              \ \n"
            - "            rm -f \\${RM_NODES_FILE} && touch \\${RM_NODES_FILE} \n"
            - "        fi \n"
            - "    fi \n"
            - "    echo \\${NODE_COUNT} >\\${NODES_COUNT_FILE} \n"
            - "} \n"
            - " \n"
            - "main() { \n"
            - "    nodes_count=\\$(cat \\${NODES_INFO_FILE} | wc -l) \n"
            - "    if [[ \"\\${nodes_count}\" == \"\\${NODE_COUNT}\" ]]; then \n"
            - "        echo \"config cluster ......\" >>\\${LOG_FILE} \n"
            - "        configCluster \n"
            - "        echo \"config finished ......\" >>\\${LOG_FILE} \n"
            - "        echo \"start cluster ......\" >>\\${LOG_FILE} \n"
            - "        startCluster \n"
            - "        echo \"start finished\" >>\\${LOG_FILE} \n"
            - "        ${ROS_NOTIFY} \n"
            - "    fi \n"
            - "} \n"
            - " \n"
            - "main \n"
            - 'EOF

              '
            - "} \n"
            - " \n"
            - "main() { \n"
            - "    prepareEnv \n"
            - "    downloadResources \n"
            - "    generateScript \n"
            - "    configLocalSSH \n"
            - "    installJavaAndConfig \n"
            - "    installAndConfigHadoop \n"
            - "    installSparkAndConfig \n"
            - "    generateClusterScript \n"
            - "    echo \"${HOST_IP}:${HOST_NAME}\" >>${NODES_INFO_FILE} \n"
            - "    touch ${RM_NODES_FILE} \n"
            - "    rm -rf /etc/hosts && touch /etc/hosts \n"
            - "} \n"
            - " \n"
            - "main \n"
            - "ros-notify -d \"{\\\"Status\\\" : \\\"Success\\\"}\" \n"
    DependsOn:
    - OOSTemplateIn
  OOSTemplateOut:
    Type: ALIYUN::OOS::Template
    Properties:
      Content:
        Fn::Join:
        - ''
        - - "FormatVersion: OOS-2019-06-01\nParameters:\n  regionId:\n    Type: String\n\
            \    Default: "
          - Ref: ALIYUN::Region
          - "\n  instanceIds:\n    Type: List\n    Default:\n      - '${instanceId}'\n\
            \  lifecycleHookId:\n    Type: String\n    Default: '${lifecycleHookId}'\n\
            \  lifecycleActionToken:\n    Type: String\n    Default: '${lifecycleActionToken}'\n\
            RamRole: "
          - Fn::GetAtt:
            - RamRole
            - RoleName
          - "\nTasks:\n  - Name: runCommand\n    Action: 'ACS::ECS::RunCommand'\n\
            \    OnError: CompleteLifecycleActionForAbandon\n    OnSuccess: CompleteLifecycleActionForContinue\n\
            \    Properties:\n      regionId: '{{ regionId }}'\n      commandContent:\
            \ |- \n"
          - Fn::Replace:
            - ros-notify:
                Fn::GetAtt:
                - RosWaitConditionHandleEss
                - CurlCli
            - Fn::Join:
              - ''
              - - "        #!/bin/sh \n"
                - "        hostname=$(hostname) \n"
                - '        MASTER_IP='
                - Fn::Select:
                  - '0'
                  - Fn::GetAtt:
                    - EcsInstanceGroupMaster
                    - PrivateIps
                - '

                  '
                - '        PASSWORD='
                - Ref: InstancePassword
                - '

                  '
                - '        NODE_COUNT='
                - Ref: Amount
                - '

                  '
                - "        echo root:${PASSWORD} | chpasswd \n"
                - "        # open sshd PasswordAuthentication \n"
                - "        sed -i 's/PasswordAuthentication no/PasswordAuthentication\
                  \ yes/g' \"/etc/ssh/sshd_config\" \n"
                - "        service sshd restart \n"
                - '        NODE_COUNT='
                - Ref: Amount
                - '

                  '
                - '        MASTER_HOSTNAME='
                - Fn::Select:
                  - '0'
                  - Fn::GetAtt:
                    - EcsInstanceGroupMaster
                    - HostNames
                - '

                  '
                - "        HOST_IP=$(ifconfig eth0 | awk '/inet /{print $2}') \n"
                - "        HOST_NAME=$(hostname) \n"
                - "        ENV_DIR=\"/software\" \n"
                - "        BASH_PATH=\"/etc/profile\" \n"
                - "        RESOURCE_DIR=\"${ENV_DIR}/resources\" \n"
                - "        set -e \n"
                - "        JDK_NAME=\"jdk-8u251-linux-i586\" \n"
                - "        JDK_RPM=\"${JDK_NAME}.rpm\" \n"
                - "        HADOOP=\"hadoop\" \n"
                - "        HADOOP_EXTRACT_NAME=\"${HADOOP}-2.7.7\" \n"
                - "        HADOOP_GZ=\"${HADOOP_EXTRACT_NAME}.tar.gz\" \n"
                - "        SPARK=\"spark\" \n"
                - "        SPARK_EXTRACT_NAME=\"${SPARK}-2.1.0-bin-hadoop2.7\" \n"
                - "        SPARK_TGZ=\"${SPARK_EXTRACT_NAME}.tgz\" \n"
                - "        SCALA=\"scala\" \n"
                - "        SCALA_EXTRACT_NAME=\"${SCALA}-2.12.1\" \n"
                - "        SCALA_TGZ=\"${SCALA_EXTRACT_NAME}.tgz\" \n"
                - "         \n"
                - "        LOG_FILE=\"${ENV_DIR}/userdata.log\" \n"
                - "        CLUSTER_FILE=\"${ENV_DIR}/cluster.sh\" \n"
                - "        SSH_SCRIPT_FILE=\"${ENV_DIR}/ssh.sh\" \n"
                - "        RM_NODES_FILE=\"${ENV_DIR}/rm_nodes.sh\" \n"
                - "        RM_NODES_INI=\"${ENV_DIR}/rm_nodes.ini\" \n"
                - "        ADD_NODES_FILE=\"${ENV_DIR}/add_nodes.sh\" \n"
                - "        NODES_INFO_FILE=\"${ENV_DIR}/nodes_info.ini\" \n"
                - "         \n"
                - "        HADOOP_HOME=${ENV_DIR}/hadoop \n"
                - "         \n"
                - "        recordLog() { \n"
                - "            time=$(date \"+%Y-%m-%d %H:%M:%S\") \n"
                - "            if [ ! -d ${ENV_DIR} ]; then \n"
                - "                mkdir ${ENV_DIR} \n"
                - "            fi \n"
                - "            echo \"$time --- $1\" >>${LOG_FILE} \n"
                - "        } \n"
                - "          \n"
                - "        createDir() { \n"
                - "            dir=$1 \n"
                - "            if [ -d \"${dir}\" ]; then \n"
                - "                recordLog \"Create failed, dir-${dir} is existed\"\
                  \ \n"
                - "            else \n"
                - "                mkdir -p \"${dir}\" \n"
                - "                recordLog \"Create Dir-${dir} successful\" \n"
                - "            fi \n"
                - "        } \n"
                - "         \n"
                - "        prepareEnv() { \n"
                - "            mkdir ${ENV_DIR} \n"
                - "            createDir ${RESOURCE_DIR} \n"
                - "        } \n"
                - "         \n"
                - "        generateSSHScript() { \n"
                - "            ssh-keygen -t rsa -P '' -f '/root/.ssh/id_rsa' \n"
                - "            yum -y install expect \n"
                - "            echo '#!/bin/bash' >${SSH_SCRIPT_FILE} \n"
                - "            echo 'name_or_ip=$1' >>${SSH_SCRIPT_FILE} \n"
                - "            echo 'authorized_key=$2' >>${SSH_SCRIPT_FILE} \n"
                - "            echo 'expect <<EOF' >>${SSH_SCRIPT_FILE} \n"
                - "            echo 'set timeout 150' >>${SSH_SCRIPT_FILE} \n"
                - "            echo \"spawn ssh root@\\${name_or_ip} echo \\\"\\${authorized_key}\\\
                  \" >> /root/.ssh/authorized_keys\" >>${SSH_SCRIPT_FILE} \n"
                - "            echo 'expect {' >>${SSH_SCRIPT_FILE} \n"
                - "            echo \"  \\\"*yes/no*\\\" { send \\\"yes\\n\\\"; exp_continue\
                  \ }\" >>${SSH_SCRIPT_FILE} \n"
                - "            echo \"  \\\"*password:\\\" { send \\\"${PASSWORD}\\\
                  n\\\" }\" >>${SSH_SCRIPT_FILE} \n"
                - "            echo '}' >>${SSH_SCRIPT_FILE} \n"
                - "            echo 'expect eof' >>${SSH_SCRIPT_FILE} \n"
                - "            echo 'EOF' >>${SSH_SCRIPT_FILE} \n"
                - "            chmod +x ${SSH_SCRIPT_FILE} \n"
                - "            recordLog \"Generate ${SSH_SCRIPT_FILE} successful\"\
                  \ \n"
                - "            echo '#!/bin/bash' >\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            echo 'host_ip=$1' >>\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            echo 'expect <<EOF' >>\"${SSH_SCRIPT_FILE}.login\"\
                  \ \n"
                - "            echo 'set timeout 150' >>\"${SSH_SCRIPT_FILE}.login\"\
                  \ \n"
                - "            echo \"spawn ssh root@\\${host_ip} exit;\" >>\"${SSH_SCRIPT_FILE}.login\"\
                  \ \n"
                - "            echo 'expect {' >>\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            echo \"  \\\"*yes/no*\\\" { send \\\"yes\\n\\\" }\"\
                  \ >>\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            echo '}' >>\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            echo 'expect eof' >>\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            echo 'EOF' >>\"${SSH_SCRIPT_FILE}.login\" \n"
                - "            chmod +x \"${SSH_SCRIPT_FILE}.login\" \n"
                - "            recordLog \"Generate ${SSH_SCRIPT_FILE}.login successful\"\
                  \ \n"
                - "        } \n"
                - "         \n"
                - "        configLocalSSH() { \n"
                - "            authorized_key=$(cat /root/.ssh/id_rsa.pub) \n"
                - "            bash ${SSH_SCRIPT_FILE} \"${MASTER_IP}\" \"${authorized_key}\"\
                  \ \n"
                - "            bash ${SSH_SCRIPT_FILE} \"${HOST_IP}\" \"${authorized_key}\"\
                  \ \n"
                - "            bash \"${SSH_SCRIPT_FILE}.login\" \"0.0.0.0\" \n"
                - "            bash \"${SSH_SCRIPT_FILE}.login\" \"localhost\" \n"
                - "            sed -i \"s/${MASTER_IP}/${MASTER_HOSTNAME},${MASTER_IP}/\"\
                  \ \"/root/.ssh/known_hosts\" \n"
                - "            recordLog \"Config expect-localhost successful\" \n"
                - "        } \n"
                - "         \n"
                - "          \n"
                - "        scpResources() { \n"
                - "            scp -r root@${MASTER_IP}:${RESOURCE_DIR}/* ${RESOURCE_DIR}\
                  \ \n"
                - "            recordLog \"Scp resources successful\" \n"
                - "        } \n"
                - "         \n"
                - "        installJavaAndConfig() { \n"
                - "            yum -y install glibc.i686 \n"
                - "            rpm -Uvh ${RESOURCE_DIR}/${JDK_RPM} \n"
                - "            # config \n"
                - "            export JAVA_HOME=$(find / -name jdk1.8.0_*) \n"
                - "            echo \"export JAVA_HOME=${JAVA_HOME}\" >>${BASH_PATH}\
                  \ \n"
                - "            echo \"export JRE_HOME=${JAVA_HOME}/jre\" >>${BASH_PATH}\
                  \ \n"
                - "            echo \"export CLASSPATH=.:${JAVA_HOME}/lib:${JAVA_HOME}/jre/lib\"\
                  \ >>${BASH_PATH} \n"
                - "            echo \"export PATH=${JAVA_HOME}/bin:$PATH\" >>${BASH_PATH}\
                  \ \n"
                - "            source ${BASH_PATH} \n"
                - "            recordLog \"Config java successful\" \n"
                - "        } \n"
                - "         \n"
                - "        installAndConfigHadoop(){ \n"
                - "            # download \n"
                - "            tar -zxvf ${RESOURCE_DIR}/${HADOOP_GZ} -C ${ENV_DIR}\
                  \ && cd ${ENV_DIR} && mv ${HADOOP_EXTRACT_NAME} ${HADOOP} \n"
                - "            echo \"export HADOOP_HOME=${HADOOP_HOME}\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export PATH=${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:$PATH\"\
                  \ >> ${BASH_PATH} \n"
                - "            source ${BASH_PATH} \n"
                - "            # config core-site.xml \n"
                - "            mv ${HADOOP_HOME}/etc/hadoop/core-site.xml ${HADOOP_HOME}/etc/hadoop/bak.core-site.xml\
                  \ \n"
                - '            cat >${HADOOP_HOME}/etc/hadoop/core-site.xml <<EOF

                  '
                - "        <configuration> \n"
                - "          <property> \n"
                - "            <name>fs.defaultFS</name> \n"
                - "            <value>hdfs://${MASTER_IP}:9000</value> \n"
                - "          </property> \n"
                - "          <property> \n"
                - "            <name>hadoop.tmp.dir</name> \n"
                - "            <value>${ENV_DIR}/tmp</value> \n"
                - "          </property> \n"
                - "        </configuration> \n"
                - '        EOF

                  '
                - "            # hdfs-site.xml \n"
                - "            mv ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml ${HADOOP_HOME}/etc/hadoop/bak.hdfs-site.xml\
                  \ \n"
                - '            cat >${HADOOP_HOME}/etc/hadoop/hdfs-site.xml <<EOF

                  '
                - "        <configuration> \n"
                - "          <property> \n"
                - "            <name>dfs.replication</name> \n"
                - "            <value>${NODE_COUNT}</value> \n"
                - "          </property> \n"
                - "        </configuration> \n"
                - '        EOF

                  '
                - "            # config mapred-site.xml \n"
                - '            cat >${HADOOP_HOME}/etc/hadoop/mapred-site.xml <<EOF

                  '
                - "        <configuration> \n"
                - "          <property> \n"
                - "            <name>mapreduce.framework.name</name> \n"
                - "            <value>yarn</value> \n"
                - "          </property> \n"
                - "        </configuration> \n"
                - '        EOF

                  '
                - "            # config yarn-site.xml \n"
                - "            mv ${HADOOP_HOME}/etc/hadoop/yarn-site.xml ${HADOOP_HOME}/etc/hadoop/bak.yarn-site.xml\
                  \ \n"
                - '            cat >${HADOOP_HOME}/etc/hadoop/yarn-site.xml <<EOF

                  '
                - "        <configuration> \n"
                - "          <property> \n"
                - "            <name>yarn.nodemanager.aux-services</name> \n"
                - "            <value>mapreduce_shuffle</value> \n"
                - "          </property> \n"
                - "          <property> \n"
                - "            <name>yarn.resourcemanager.hostname</name> \n"
                - "            <value>${MASTER_IP}</value> \n"
                - "          </property> \n"
                - "        </configuration> \n"
                - '        EOF

                  '
                - "            # config java home \n"
                - "            sed -i 's/export JAVA_HOME=/#export JAVA_HOME=/' ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh\
                  \ \n"
                - "            sed -i \"/#export JAVA_HOME=/a export JAVA_HOME=${JAVA_HOME}\"\
                  \ ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh \n"
                - "            sed -i 's/export JAVA_HOME=/#export JAVA_HOME=/' ${HADOOP_HOME}/etc/hadoop/mapred-env.sh\
                  \ \n"
                - "            sed -i \"/#export JAVA_HOME=/a export JAVA_HOME=${JAVA_HOME}\"\
                  \ ${HADOOP_HOME}/etc/hadoop/mapred-env.sh \n"
                - "            sed -i 's/export JAVA_HOME=/#export JAVA_HOME=/' ${HADOOP_HOME}/etc/hadoop/yarn-env.sh\
                  \ \n"
                - "            sed -i \"/#export JAVA_HOME=/a export JAVA_HOME=${JAVA_HOME}\"\
                  \ ${HADOOP_HOME}/etc/hadoop/yarn-env.sh \n"
                - "            recordLog \"Config hadoop successful\" \n"
                - "        } \n"
                - "         \n"
                - "        installSparkAndConfig() { \n"
                - "           tar zxvf ${RESOURCE_DIR}/${SPARK_TGZ} -C ${ENV_DIR}\
                  \ && cd ${ENV_DIR} && mv ${SPARK_EXTRACT_NAME} ${SPARK} \n"
                - "           mv ${ENV_DIR}/${SPARK}/conf/spark-env.sh.template ${ENV_DIR}/${SPARK}/conf/spark-env.sh\
                  \ \n"
                - "           echo \"export SCALA_HOME=${ENV_DIR}/scala\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
                  \ \n"
                - "           echo \"export JAVA_HOME=${JAVA_HOME}\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
                  \ \n"
                - "           echo \"export HADOOP_HOME=${ENV_DIR}/hadoop\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
                  \ \n"
                - "           echo \"export HADOOP_CONF_DIR=${ENV_DIR}/hadoop/etc/hadoop\"\
                  \ >> ${ENV_DIR}/spark/conf/spark-env.sh \n"
                - "           echo \"export SPARK_HOME=${ENV_DIR}/spark\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
                  \ \n"
                - "           echo \"export SPARK_MASTER_IP=${HOST_IP}\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
                  \ \n"
                - "           echo \"export SPARK_EXECUTOR_MEMORY=1G\" >> ${ENV_DIR}/spark/conf/spark-env.sh\
                  \ \n"
                - "           recordLog \"Config spark successful\" \n"
                - "        } \n"
                - "         \n"
                - "        installScalaAndConfig() { \n"
                - "            tar zxvf ${RESOURCE_DIR}/${SCALA_TGZ} -C ${ENV_DIR}\
                  \ && cd ${ENV_DIR} && mv ${SCALA_EXTRACT_NAME} ${SCALA} \n"
                - "            echo \"export SCALA_HOME=${ENV_DIR}/scala\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"PATH=$PATH:${ENV_DIR}/scala/bin\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export JAVA_HOME=${JAVA_HOME}\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export JRE_HOME=${JAVA_HOME}/jre\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\"\
                  \ >> ${BASH_PATH} \n"
                - "            echo \"export PATH=${JAVA_HOME}/bin:$PATH\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export HADOOP_HOME=${ENV_DIR}/hadoop\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export PATH=.:${ENV_DIR}/hadoop/bin:$PATH\"\
                  \ >> ${BASH_PATH} \n"
                - "            echo \"export SPARK_HOME=${ENV_DIR}/spark\" >> ${BASH_PATH}\
                  \ \n"
                - "            echo \"export PATH=.:${ENV_DIR}/spark/bin:$PATH\" >>\
                  \ ${BASH_PATH} \n"
                - "            source ${BASH_PATH} \n"
                - "            recordLog \"Config scala successful\" \n"
                - "        } \n"
                - "         \n"
                - "        generateAddNodeScript() { \n"
                - "            echo '#!/bin/bash' >${ADD_NODES_FILE} \n"
                - "            echo 'NODE_COUNT=$1' >>${ADD_NODES_FILE} \n"
                - "            echo \"ssh root@${MASTER_IP} \\\"echo '${HOST_IP}:${HOST_NAME}'\
                  \ >> ${NODES_INFO_FILE};bash ${CLUSTER_FILE} \\${NODE_COUNT}\\\"\
                  \" >> ${ADD_NODES_FILE} \n"
                - "        } \n"
                - "         \n"
                - "        generateRmNodeScript() { \n"
                - "            echo '#!/bin/bash' >${RM_NODES_FILE} \n"
                - "            echo 'NODE_COUNT=$1' >>${RM_NODES_FILE} \n"
                - "            echo \"ssh root@${MASTER_IP} \\\"sed -i '/${HOST_IP}:${HOST_NAME}/d'\
                  \ ${NODES_INFO_FILE};echo ${HOST_IP}:${HOST_NAME} >> ${RM_NODES_INI};bash\
                  \ ${CLUSTER_FILE} \\${NODE_COUNT}\\\"\" >>${RM_NODES_FILE} \n"
                - "        } \n"
                - "         \n"
                - "        main() { \n"
                - "            prepareEnv \n"
                - "            generateSSHScript \n"
                - "            configLocalSSH \n"
                - "            scpResources \n"
                - "            installJavaAndConfig \n"
                - "            installAndConfigHadoop \n"
                - "            installSparkAndConfig \n"
                - "            installScalaAndConfig \n"
                - "            generateAddNodeScript \n"
                - "            generateRmNodeScript \n"
                - "            bash ${ADD_NODES_FILE} ${NODE_COUNT} \n"
                - "        } \n"
                - "         \n"
                - "        main \n"
                - "        ros-notify \n"
          - |2-

                  instanceId: '{{ ACS::TaskLoopItem }}'
                  commandType: RunShellScript
                Loop:
                  RateControl:
                    Mode: Concurrency
                    MaxErrors: 0
                    Concurrency: 10
                  Items: '{{ instanceIds }}'
                  Outputs:
                    commandOutputs:
                      AggregateType: 'Fn::ListJoin'
                      AggregateField: commandOutput
                Outputs:
                  commandOutput:
                    Type: String
                    ValueSelector: invocationOutput
              - Name: CompleteLifecycleActionForContinue
                Action: 'ACS::ExecuteAPI'
                OnSuccess: 'ACS::END'
                Properties:
                  Service: ESS
                  API: CompleteLifecycleAction
                  Parameters:
                    RegionId: '{{ regionId }}'
                    LifecycleHookId: '{{ lifecycleHookId }}'
                    LifecycleActionToken: '{{ lifecycleActionToken }}'
              - Name: CompleteLifecycleActionForAbandon
                Action: 'ACS::ExecuteAPI'
                Properties:
                  Service: ESS
                  API: CompleteLifecycleAction
                  Parameters:
                    RegionId: '{{ regionId }}'
                    LifecycleHookId: '{{ lifecycleHookId }}'
                    LifecycleActionToken: '{{ lifecycleActionToken }}'
                    LifecycleActionResult: ABANDON
      TemplateName:
        Fn::Join:
        - ''
        - - StackId-
          - Ref: ALIYUN::StackId
          - -Out
    DependsOn:
    - RamRole
  RosWaitConditionMasterHandle:
    Type: ALIYUN::ROS::WaitConditionHandle
    Properties: {}
  RosWaitConditionMaster:
    Type: ALIYUN::ROS::WaitCondition
    Properties:
      Count: 1
      Handle:
        Ref: RosWaitConditionMasterHandle
      Timeout: 1800
  EssScalingGroupSlave:
    Type: ALIYUN::ESS::ScalingGroup
    Properties:
      VSwitchId:
        Ref: VSwitchId
      DefaultCooldown: 0
      DesiredCapacity:
        Fn::Calculate:
        - '{0}-1'
        - 0
        - - Ref: Amount
      HealthCheckType: ECS
      MaxSize:
        Ref: Amount
      MinSize: 2
      RemovalPolicys:
      - NewestInstance
      - OldestScalingConfiguration
      ScalingGroupName:
        Fn::Join:
        - '-'
        - - Spark-ros
          - Ref: ALIYUN::StackId
          - Ref: Amount
    DependsOn:
    - EcsInstanceGroupMaster
    - OOSTemplateIn
    - OOSTemplateOut
    - RosWaitConditionMaster
  EssLifecycleHookIn:
    Type: ALIYUN::ESS::LifecycleHook
    Properties:
      DefaultResult: CONTINUE
      HeartbeatTimeout: 600
      LifecycleTransition: SCALE_IN
      NotificationArn:
        Fn::Join:
        - ''
        - - 'acs:ess:'
          - Ref: ALIYUN::Region
          - ':'
          - Ref: ALIYUN::TenantId
          - :oos/
          - Fn::GetAtt:
            - OOSTemplateIn
            - TemplateName
      NotificationMetadata:
        Fn::Join:
        - ''
        - - '{"regionId": "${regionId}","instanceIds": "${instanceIds}","lifecycleHookId":
            "${lifecycleHookId}","lifecycleActionToken": "${lifecycleActionToken}"}'
      ScalingGroupId:
        Ref: EssScalingGroupSlave
    DependsOn:
    - EssScalingGroupSlave
    - OOSTemplateIn
    - OOSTemplateOut
  EssLifecycleHookOut:
    Type: ALIYUN::ESS::LifecycleHook
    Properties:
      DefaultResult: CONTINUE
      HeartbeatTimeout: 600
      LifecycleTransition: SCALE_OUT
      NotificationArn:
        Fn::Join:
        - ''
        - - 'acs:ess:'
          - Ref: ALIYUN::Region
          - ':'
          - Ref: ALIYUN::TenantId
          - :oos/
          - Fn::GetAtt:
            - OOSTemplateOut
            - TemplateName
      NotificationMetadata:
        Fn::Join:
        - ''
        - - '{"regionId": "${regionId}","instanceIds": "${instanceIds}","lifecycleHookId":
            "${lifecycleHookId}","lifecycleActionToken": "${lifecycleActionToken}"}'
      ScalingGroupId:
        Ref: EssScalingGroupSlave
    DependsOn:
    - EssScalingGroupSlave
    - OOSTemplateOut
  EssScalingConfigurationSlave:
    Type: ALIYUN::ESS::ScalingConfiguration
    Properties:
      SecurityGroupId:
        Ref: SecurityGroupId
      ImageId: centos_7_06_64_20G_alibase_20190711.vhd
      InstanceName:
        Fn::Join:
        - '-'
        - - Spark-node
          - Ref: ALIYUN::StackId
      InstanceTypes:
      - Ref: InstanceType
      IoOptimized: optimized
      ScalingConfigurationName:
        Fn::Join:
        - '-'
        - - Spark-ros
          - Ref: ALIYUN::StackId
          - Ref: Amount
      ScalingGroupId:
        Ref: EssScalingGroupSlave
      SystemDiskCategory:
        Ref: DiskCategory
      SystemDiskSize:
        Ref: DiskSize
    DependsOn:
    - EssScalingGroupSlave
  EssScalingGroupEnable:
    Type: ALIYUN::ESS::ScalingGroupEnable
    Properties:
      ScalingConfigurationId:
        Ref: EssScalingConfigurationSlave
      ScalingGroupId:
        Ref: EssScalingGroupSlave
    DependsOn:
    - EcsInstanceGroupMaster
    - EssScalingConfigurationSlave
    - RosWaitConditionMaster
  RosWaitConditionClusterHandle:
    Type: ALIYUN::ROS::WaitConditionHandle
    Properties: {}
  RosWaitConditionCluster:
    Type: ALIYUN::ROS::WaitCondition
    Properties:
      Count: 1
      Handle:
        Ref: RosWaitConditionClusterHandle
      Timeout: 3600
  RosWaitConditionHandleEss:
    Type: ALIYUN::ROS::WaitConditionHandle
    Properties: {}
  RosWaitConditionEss:
    Type: ALIYUN::ROS::WaitCondition
    Properties:
      Count:
        Fn::Calculate:
        - '{0}-1'
        - 0
        - - Ref: Amount
      Handle:
        Ref: RosWaitConditionHandleEss
      Timeout: 1800
Outputs:
  EcsInstanceIds:
    Value:
      Fn::GetAtt:
      - EcsInstanceGroupMaster
      - InstanceIds
  EssGroupId:
    Value:
      Fn::GetAtt:
      - EssScalingGroupSlave
      - ScalingGroupId
  MasterPrivateIp:
    Value:
      Fn::Select:
      - '0'
      - Fn::GetAtt:
        - EcsInstanceGroupMaster
        - PrivateIps
  SparkWebSiteURL:
    Value:
      'Fn::Join':
        - ''
        - - 'http://'
          - 'Fn::Select':
              - '0'
              - 'Fn::GetAtt':
                  - EcsInstanceGroupMaster
                  - PublicIps
          - ':8080'
Metadata:
  ALIYUN::ROS::Interface:
    ParameterGroups:
    - Parameters:
      - VpcId
      - VSwitchZoneId
      - VSwitchId
      - SecurityGroupId
      Label:
        default:
          en: Infrastructure Configuration
          zh-cn: 基础资源配置（必填）
    - Parameters:
      - InstanceType
      - InstancePassword
      - DiskCategory
      - DiskSize
      - Amount
      Label:
        default:
          en: Spark Configuration
          zh-cn: Spark 配置（必填）
    TemplateTags:
    - acs:solution:数据分析:Spark集群版(已有VPC)
  ALIYUN::ROS::Composer:
    58fec509:
      Rect:
        - 692
        - 530
        - 40
        - 100
        - 1
        - 0
      ResT: Composer::ROSParameter::AlibabaCloud
    c62f7d1d:
      Parent: 58fec509
      Rect:
        - 656
        - 460
        - 60
        - 150
        - 2
        - 0
      ResT: Composer::ROSParameter::Region
    5ed4daa3:
      Res:
        - RamRole
      Parent: c62f7d1d
      Rect:
        - 40
        - 40
        - 660
        - 200
        - 3
        - 0
      Hidden: true
    4e0c6832:
      Res:
        - OOSTemplateIn
      Parent: c62f7d1d
      Rect:
        - 40
        - 40
        - 540
        - 200
        - 3
        - 0
      Hidden: true
    863310d4:
      Res:
        - OOSTemplateOut
      Parent: c62f7d1d
      Rect:
        - 40
        - 40
        - 600
        - 200
        - 3
        - 0
      Hidden: true
    d7d73dc9:
      Res:
        - RosWaitConditionMasterHandle
      Parent: c62f7d1d
      Rect:
        - 40
        - 40
        - 660
        - 490
        - 3
        - 0
      Hidden: true
    4335f2c9:
      Res:
        - RosWaitConditionMaster
      Parent: c62f7d1d
      Rect:
        - 40
        - 40
        - 80
        - 490
        - 3
        - 0
      Hidden: true
    a2b28d93:
      Res:
        - RosWaitConditionClusterHandle
      Parent: c62f7d1d
      Rect:
        - 40
        - 40
        - 80
        - 550
        - 3
        - 0
      Hidden: true
    5773ac65:
      Res:
        - RosWaitConditionCluster
      Parent: c62f7d1d
      Rect:
        - 40
        - 40
        - 540
        - 490
        - 3
        - 0
      Hidden: true
    935a1bfd:
      Res:
        - RosWaitConditionHandleEss
      Parent: c62f7d1d
      Rect:
        - 40
        - 40
        - 540
        - 550
        - 3
        - 0
      Hidden: true
    723f9eea:
      Res:
        - RosWaitConditionEss
      Parent: c62f7d1d
      Rect:
        - 40
        - 40
        - 600
        - 490
        - 3
        - 0
      Hidden: true
    ca260f8b:
      Res:
        - VpcId
      Parent: c62f7d1d
      Rect:
        - 567
        - 349
        - 80
        - 200
        - 3
        - 0
      ResT: ALIYUN::ECS::VPC
    51fefcdf:
      Res:
        - VSwitchZoneId
      Parent: ca260f8b
      Rect:
        - 493
        - 275
        - 120
        - 240
        - 3
        - 0
      ResT: Composer::ROSParameter::Zone
    b1f8bb06:
      Res:
        - VSwitchId
      Parent: ca260f8b
      Rect:
        - 400
        - 200
        - 170
        - 280
        - 4
        - 0
      ResT: ALIYUN::ECS::VSwitch
    d7836d54:
      Edge:
        - 4e0c6832
        - 5ed4daa3
      Line: 0:0:0:gray:0
    35eaebe1:
      Edge:
        - 863310d4
        - 5ed4daa3
      Line: 0:0:0:gray:0
    03bb43b5:
      Parent: c62f7d1d
      Edge:
        - 863310d4
        - 30c61a9c
      Line: 0:0:0:gray:0
    9fa004e2:
      Edge:
        - 4335f2c9
        - d7d73dc9
      Line: 0:0:0:gray:0
    d8456d61:
      Parent: c62f7d1d
      Edge:
        - 86b448bc
        - 4e0c6832
      Line: 0:0:0:gray:0
    75bdebfb:
      Parent: c62f7d1d
      Edge:
        - 86b448bc
        - 863310d4
      Line: 0:0:0:gray:0
    70d59aa7:
      Edge:
        - 5773ac65
        - a2b28d93
      Line: 0:0:0:gray:0
    b6594f0d:
      Edge:
        - 723f9eea
        - 935a1bfd
      Line: 0:0:0:gray:0
    86b448bc:
      Res:
        - EssScalingGroupSlave
        - EssLifecycleHookIn
        - EssLifecycleHookOut
        - EssScalingConfigurationSlave
        - EssScalingGroupEnable
      Parent: ca260f8b
      Rect:
        - 248
        - 100
        - 246
        - 334
        - 11
        - 0
      Label:
        zh-CN: 云服务器弹性伸缩组
    30c61a9c:
      Res:
        - EcsInstanceGroupMaster
      Parent: b1f8bb06
      Rect:
        - 40
        - 40
        - 343
        - 360
        - 12
        - 0
      Layer:
        - 86b448bc
      Label:
        zh-CN: 主节点云服务器
